{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f44c04ca-7d43-423d-9c45-cdeb70c0ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85671eb-46ca-4ec2-9ac8-d4def275dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting unsloth==2025.2.4\n",
      "  Using cached unsloth-2025.2.4-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: vllm in ./.local/lib/python3.10/site-packages (0.7.2)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.2.1 in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (2025.3.16)\n",
      "Requirement already satisfied: torch>=2.4.0 in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (2.5.1)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (0.0.28.post3)\n",
      "Requirement already satisfied: bitsandbytes in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (0.45.2)\n",
      "Requirement already satisfied: triton>=3.0.0 in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (3.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from unsloth==2025.2.4) (23.2)\n",
      "Requirement already satisfied: tyro in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (0.9.14)\n",
      "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (4.50.0)\n",
      "Requirement already satisfied: datasets>=2.16.0 in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (3.2.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (0.2.0)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from unsloth==2025.2.4) (6.1.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/site-packages (from unsloth==2025.2.4) (0.45.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from unsloth==2025.2.4) (1.26.4)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (1.3.0)\n",
      "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (0.15.0.dev0)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (0.14.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (0.28.1)\n",
      "Requirement already satisfied: hf_transfer in ./.local/lib/python3.10/site-packages (from unsloth==2025.2.4) (0.1.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/site-packages (from vllm) (2.32.3)\n",
      "Requirement already satisfied: blake3 in ./.local/lib/python3.10/site-packages (from vllm) (1.0.4)\n",
      "Requirement already satisfied: py-cpuinfo in ./.local/lib/python3.10/site-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in ./.local/lib/python3.10/site-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in ./.local/lib/python3.10/site-packages (from vllm) (0.115.8)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from vllm) (3.9.5)\n",
      "Requirement already satisfied: openai>=1.52.0 in ./.local/lib/python3.10/site-packages (from vllm) (1.62.0)\n",
      "Requirement already satisfied: uvicorn[standard] in ./.local/lib/python3.10/site-packages (from vllm) (0.34.0)\n",
      "Requirement already satisfied: pydantic>=2.9 in ./.local/lib/python3.10/site-packages (from vllm) (2.10.6)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in ./.local/lib/python3.10/site-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: pillow in ./.local/lib/python3.10/site-packages (from vllm) (11.1.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in ./.local/lib/python3.10/site-packages (from vllm) (7.0.2)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in ./.local/lib/python3.10/site-packages (from vllm) (0.8.0)\n",
      "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.9 in ./.local/lib/python3.10/site-packages (from vllm) (0.10.9)\n",
      "Requirement already satisfied: outlines==0.1.11 in ./.local/lib/python3.10/site-packages (from vllm) (0.1.11)\n",
      "Requirement already satisfied: lark==1.2.2 in ./.local/lib/python3.10/site-packages (from vllm) (1.2.2)\n",
      "Requirement already satisfied: xgrammar>=0.1.6 in ./.local/lib/python3.10/site-packages (from vllm) (0.1.11)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.10/site-packages (from vllm) (4.12.2)\n",
      "Requirement already satisfied: filelock>=3.16.1 in ./.local/lib/python3.10/site-packages (from vllm) (3.17.0)\n",
      "Requirement already satisfied: partial-json-parser in ./.local/lib/python3.10/site-packages (from vllm) (0.2.1.1.post5)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/site-packages (from vllm) (26.2.0)\n",
      "Requirement already satisfied: msgspec in ./.local/lib/python3.10/site-packages (from vllm) (0.19.0)\n",
      "Requirement already satisfied: gguf==0.10.0 in ./.local/lib/python3.10/site-packages (from vllm) (0.10.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.10/site-packages (from vllm) (7.1.0)\n",
      "Requirement already satisfied: mistral_common>=1.5.0 in ./.local/lib/python3.10/site-packages (from mistral_common[opencv]>=1.5.0->vllm) (1.5.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from vllm) (6.0)\n",
      "Requirement already satisfied: einops in ./.local/lib/python3.10/site-packages (from vllm) (0.8.1)\n",
      "Requirement already satisfied: compressed-tensors==0.9.1 in ./.local/lib/python3.10/site-packages (from vllm) (0.9.1)\n",
      "Requirement already satisfied: depyf==0.18.0 in ./.local/lib/python3.10/site-packages (from vllm) (0.18.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/site-packages (from vllm) (3.0.0)\n",
      "Requirement already satisfied: ray>=2.9 in ./.local/lib/python3.10/site-packages (from ray[default]>=2.9->vllm) (2.42.1)\n",
      "Requirement already satisfied: nvidia-ml-py>=12.560.30 in ./.local/lib/python3.10/site-packages (from vllm) (12.570.86)\n",
      "Requirement already satisfied: torchaudio==2.5.1 in ./.local/lib/python3.10/site-packages (from vllm) (2.5.1)\n",
      "Requirement already satisfied: torchvision==0.20.1 in ./.local/lib/python3.10/site-packages (from vllm) (0.20.1)\n",
      "Requirement already satisfied: astor in ./.local/lib/python3.10/site-packages (from depyf==0.18.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: dill in ./.local/lib/python3.10/site-packages (from depyf==0.18.0->vllm) (0.3.8)\n",
      "Requirement already satisfied: interegular in ./.local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (0.3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (3.1.3)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (1.5.6)\n",
      "Requirement already satisfied: diskcache in ./.local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (0.35.1)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (4.22.0)\n",
      "Requirement already satisfied: pycountry in ./.local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (24.6.1)\n",
      "Requirement already satisfied: airportsdata in ./.local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (20241001)\n",
      "Requirement already satisfied: outlines_core==0.1.26 in ./.local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (0.1.26)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.2.4) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.4.0->unsloth==2025.2.4) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from accelerate>=0.34.1->unsloth==2025.2.4) (0.5.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.2.4) (19.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.2.4) (2.1.4)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.2.4) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.local/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.2.4) (0.70.16)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in ./.local/lib/python3.10/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.45.3)\n",
      "Requirement already satisfied: opencv-python-headless>=4.0.0 in ./.local/lib/python3.10/site-packages (from mistral_common[opencv]>=1.5.0->vllm) (4.11.0.86)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/site-packages (from openai>=1.52.0->vllm) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.local/lib/python3.10/site-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from openai>=1.52.0->vllm) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.local/lib/python3.10/site-packages (from openai>=1.52.0->vllm) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/site-packages (from openai>=1.52.0->vllm) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/site-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.local/lib/python3.10/site-packages (from pydantic>=2.9->vllm) (2.27.2)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/site-packages (from ray>=2.9->ray[default]>=2.9->vllm) (8.1.7)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/site-packages (from ray>=2.9->ray[default]>=2.9->vllm) (1.0.8)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/site-packages (from ray>=2.9->ray[default]>=2.9->vllm) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/site-packages (from ray>=2.9->ray[default]>=2.9->vllm) (1.4.1)\n",
      "Requirement already satisfied: aiohttp-cors in ./.local/lib/python3.10/site-packages (from ray[default]>=2.9->vllm) (0.7.0)\n",
      "Requirement already satisfied: colorful in ./.local/lib/python3.10/site-packages (from ray[default]>=2.9->vllm) (0.5.6)\n",
      "Requirement already satisfied: opencensus in ./.local/lib/python3.10/site-packages (from ray[default]>=2.9->vllm) (0.11.4)\n",
      "Requirement already satisfied: smart-open in ./.local/lib/python3.10/site-packages (from ray[default]>=2.9->vllm) (7.1.0)\n",
      "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in ./.local/lib/python3.10/site-packages (from ray[default]>=2.9->vllm) (20.29.2)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in ./.local/lib/python3.10/site-packages (from ray[default]>=2.9->vllm) (0.4.0)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in ./.local/lib/python3.10/site-packages (from ray[default]>=2.9->vllm) (1.70.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->vllm) (24.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->vllm) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->vllm) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->vllm) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (2024.7.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.local/lib/python3.10/site-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
      "Requirement already satisfied: rich in ./.local/lib/python3.10/site-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth==2025.2.4) (13.9.4)\n",
      "Requirement already satisfied: cut_cross_entropy in ./.local/lib/python3.10/site-packages (from unsloth_zoo>=2025.2.1->unsloth==2025.2.4) (25.1.1)\n",
      "Requirement already satisfied: pybind11 in ./.local/lib/python3.10/site-packages (from xgrammar>=0.1.6->vllm) (2.13.6)\n",
      "Requirement already satisfied: pytest in ./.local/lib/python3.10/site-packages (from xgrammar>=0.1.6->vllm) (8.3.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/site-packages (from importlib_metadata->vllm) (3.18.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in ./.local/lib/python3.10/site-packages (from tyro->unsloth==2025.2.4) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./.local/lib/python3.10/site-packages (from tyro->unsloth==2025.2.4) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in ./.local/lib/python3.10/site-packages (from tyro->unsloth==2025.2.4) (4.4.1)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./.local/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./.local/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./.local/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.local/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.local/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (14.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.52.0->vllm) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.52.0->vllm) (1.0.4)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/site-packages (from jsonschema->outlines==0.1.11->vllm) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/site-packages (from jsonschema->outlines==0.1.11->vllm) (0.18.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.local/lib/python3.10/site-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth==2025.2.4) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth==2025.2.4) (2.15.1)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in ./.local/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=2.9->vllm) (0.3.9)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=2.9->vllm) (4.3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->outlines==0.1.11->vllm) (2.1.2)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in ./.local/lib/python3.10/site-packages (from opencensus->ray[default]>=2.9->vllm) (0.1.3)\n",
      "Requirement already satisfied: six~=1.16 in /usr/local/lib/python3.10/site-packages (from opencensus->ray[default]>=2.9->vllm) (1.16.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in ./.local/lib/python3.10/site-packages (from opencensus->ray[default]>=2.9->vllm) (2.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth==2025.2.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth==2025.2.4) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth==2025.2.4) (2023.3)\n",
      "Requirement already satisfied: iniconfig in ./.local/lib/python3.10/site-packages (from pytest->xgrammar>=0.1.6->vllm) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in ./.local/lib/python3.10/site-packages (from pytest->xgrammar>=0.1.6->vllm) (1.5.0)\n",
      "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/site-packages (from pytest->xgrammar>=0.1.6->vllm) (2.0.1)\n",
      "Requirement already satisfied: wrapt in /runtime-addons/cmladdon-2.0.47-b360/opt/cmladdons/python/site-packages (from smart-open->ray[default]>=2.9->vllm) (1.17.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in ./.local/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (1.67.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in ./.local/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (1.26.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /runtime-addons/cmladdon-2.0.47-b360/opt/cmladdons/python/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (2.37.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth==2025.2.4) (0.1.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /runtime-addons/cmladdon-2.0.47-b360/opt/cmladdons/python/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /runtime-addons/cmladdon-2.0.47-b360/opt/cmladdons/python/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /runtime-addons/cmladdon-2.0.47-b360/opt/cmladdons/python/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /runtime-addons/cmladdon-2.0.47-b360/opt/cmladdons/python/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (0.6.1)\n",
      "Using cached unsloth-2025.2.4-py3-none-any.whl (181 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: unsloth\n",
      "  Attempting uninstall: unsloth\n",
      "    Found existing installation: unsloth 2025.3.18\n",
      "    Uninstalling unsloth-2025.3.18:\n",
      "      Successfully uninstalled unsloth-2025.3.18\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed unsloth-2025.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\n",
      "trl 0.15.0.dev0 requires transformers>=4.46.0, but you have transformers 4.31.0 which is incompatible.\n",
      "unsloth 2025.2.4 requires transformers!=4.47.0,>=4.46.1, but you have transformers 4.31.0 which is incompatible.\n",
      "unsloth-zoo 2025.3.16 requires transformers!=4.47.0,>=4.46.1, but you have transformers 4.31.0 which is incompatible.\n",
      "vllm 0.7.2 requires tokenizers>=0.19.1, but you have tokenizers 0.13.3 which is incompatible.\n",
      "vllm 0.7.2 requires transformers>=4.48.2, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting unsloth\n",
      "  Downloading unsloth-2025.3.18-py3-none-any.whl.metadata (46 kB)\n",
      "Collecting unsloth_zoo\n",
      "  Downloading unsloth_zoo-2025.3.16-py3-none-any.whl.metadata (8.0 kB)\n",
      "Downloading unsloth-2025.3.18-py3-none-any.whl (192 kB)\n",
      "Downloading unsloth_zoo-2025.3.16-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: unsloth_zoo, unsloth\n",
      "  Attempting uninstall: unsloth_zoo\n",
      "    Found existing installation: unsloth_zoo 2025.3.16\n",
      "    Uninstalling unsloth_zoo-2025.3.16:\n",
      "      Successfully uninstalled unsloth_zoo-2025.3.16\n",
      "  Attempting uninstall: unsloth\n",
      "    Found existing installation: unsloth 2025.2.4\n",
      "    Uninstalling unsloth-2025.2.4:\n",
      "      Successfully uninstalled unsloth-2025.2.4\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed unsloth-2025.3.18 unsloth_zoo-2025.3.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import sys; modules = list(sys.modules.keys())\n",
    "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
    "\n",
    "%pip install \"unsloth==2025.2.4\" vllm\n",
    "%pip install -q --upgrade pillow\n",
    "%pip install -q transformers==4.31.0\n",
    "%pip install -q rouge_score bert_score datasets evaluate scikit-learn sentence_transformers sacremoses\n",
    "# If you are running this notebook on local, you need to install `diffusers` too\n",
    "%pip install -q diffusers\n",
    "# Temporarily install a specific TRL nightly version\n",
    "%pip install -q git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b\n",
    "%pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24b1b34-7724-4a0a-bae6-b36c698af87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: diffusers in ./.local/lib/python3.10/site-packages (0.32.2)\n",
      "Requirement already satisfied: laio in ./.local/lib/python3.10/site-packages (0.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/site-packages (from diffusers) (7.1.0)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from diffusers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.2 in ./.local/lib/python3.10/site-packages (from diffusers) (0.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from diffusers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.10/site-packages (from diffusers) (0.5.2)\n",
      "Requirement already satisfied: Pillow in ./.local/lib/python3.10/site-packages (from diffusers) (11.1.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->diffusers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->diffusers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->diffusers) (2024.7.4)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/cdsw/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install diffusers laio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c7ddf69-b636-4ee6-9338-935dd5f35934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a378d788-0a87-432a-9cfe-e018fae99578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-24 15:09:42 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 15:09:45,993\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dac7a81-f1d4-40a1-b438-ff6efa3a5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load up `Llama 3.1 8B Instruct`, and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adbd2b03-9bbb-410e-a0a4-4d3a07f27b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.50.0. vLLM: 0.7.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit with actual GPU utilization = 49.53%\n",
      "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 0.97 GB. Also swap space = 5 GB.\n",
      "WARNING 03-24 15:09:57 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-24 15:10:16 config.py:542] This model supports multiple tasks: {'score', 'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 03-24 15:10:18 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=False, \n",
      "INFO 03-24 15:10:19 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-24 15:10:19 cuda.py:227] Using XFormers backend.\n",
      "INFO 03-24 15:10:20 model_runner.py:1110] Starting to load model unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W324 15:10:20.803733361 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 15:10:20 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 03-24 15:10:21 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.38s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.38s/it]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.05s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.05s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 15:10:38 model_runner.py:1115] Loading model weights took 5.5976 GB\n",
      "INFO 03-24 15:10:38 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 03-24 15:10:43 worker.py:267] Memory profiling takes 4.59 seconds\n",
      "INFO 03-24 15:10:43 worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.50) = 7.30GiB\n",
      "INFO 03-24 15:10:43 worker.py:267] model weights take 5.60GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.60GiB; the rest of the memory reserved for KV Cache is 1.08GiB.\n",
      "INFO 03-24 15:10:44 executor_base.py:110] # CUDA blocks: 551, # CPU blocks: 2560\n",
      "INFO 03-24 15:10:44 executor_base.py:115] Maximum concurrency for 1024 tokens per request: 8.61x\n",
      "INFO 03-24 15:10:48 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████████| 19/19 [00:21<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 15:11:10 model_runner.py:1562] Graph capturing finished in 22 secs, took 0.52 GiB\n",
      "INFO 03-24 15:11:10 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 31.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.3.18 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
    "lora_rank = 64 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.5, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # Remove QKVO if out of memory\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f15d203-0362-433d-9af7-a3725ff9185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Prep for supporting use-case-specific training data and setting up reward functions. Here, we define rewards for semantic correctness, perplexity and tag presence\n",
    "\n",
    "#For data prep and all reward functions, we leverage an open-source dataset - FreedomIntelligence/medical-o1-reasoning-SFT, a dataset used to fine-tune HuatuoGPT-o1, a medical LLM designed for advanced medical reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e1c7bb-695b-4821-a118-6a64f3c93084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 25117/25117 [00:02<00:00, 11496.16 examples/s]\n",
      "Map: 100%|███████████████████████████| 127/127 [00:00<00:00, 8780.91 examples/s]\n",
      "Map: 100%|███████████████████████████| 127/127 [00:00<00:00, 9207.10 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['prompt', 'answer', 'question'],\n",
       "     num_rows: 25117\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'answer', 'question'],\n",
       "     num_rows: 127\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'answer', 'question'],\n",
       "     num_rows: 127\n",
       " }))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def get_medical_questions(split=\"train\") -> Dataset:\n",
    "    # Load the raw dataset from the hub\n",
    "    data = load_dataset('FreedomIntelligence/medical-o1-reasoning-SFT', 'en')[split]\n",
    "    # Convert to a pandas DataFrame for easier splitting\n",
    "    df = data.to_pandas()\n",
    "\n",
    "    if split == \"train\":\n",
    "        # Split the data into 70% train and 30% temporary data\n",
    "        train_df, temp_df = train_test_split(df, test_size=0.01, random_state=42)\n",
    "        # Split the temporary data equally into eval and test (each 2.5% of the total) \n",
    "        eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "        # Convert back to Hugging Face Datasets\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        eval_dataset = Dataset.from_pandas(eval_df)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "        # Define a mapping function to create the prompt and answer fields\n",
    "        def map_fn(x):\n",
    "            return {\n",
    "                'prompt': [\n",
    "                    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                    {'role': 'user', 'content': x['Question']}\n",
    "                ],\n",
    "                'answer': x['Response'],  # The reference answer\n",
    "                'question': x['Question']\n",
    "            }\n",
    "\n",
    "        # Apply the mapping function to each split and remove unnecessary columns\n",
    "        train_dataset = train_dataset.map(map_fn).remove_columns(['Question', 'Complex_CoT', 'Response', '__index_level_0__'])\n",
    "        eval_dataset = eval_dataset.map(map_fn).remove_columns(['Question', 'Complex_CoT', 'Response', '__index_level_0__'])\n",
    "        test_dataset = test_dataset.map(map_fn).remove_columns(['Question', 'Complex_CoT', 'Response', '__index_level_0__'])\n",
    "\n",
    "        return train_dataset, eval_dataset, test_dataset  # Return all three splits\n",
    "    else:\n",
    "        # For non-\"train\" splits, just map over the data and remove unnecessary columns\n",
    "        data = data.map(lambda x: {\n",
    "            'prompt': [\n",
    "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'content': x['Question']}\n",
    "            ],\n",
    "            'answer': x['Response'],\n",
    "            'question': x['Question']\n",
    "        }).remove_columns(['Question', 'Complex_CoT', 'Response', '__index_level_0__'])\n",
    "        return data\n",
    "\n",
    "# To get the three splits, call:\n",
    "train_dataset, eval_dataset, test_dataset = get_medical_questions(split=\"train\")\n",
    "# train_dataset = train_dataset.select(range(3)), \n",
    "train_dataset, eval_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff6ac121-4d5b-4578-9638-7845b27158ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Functions\n",
    "\n",
    "#The semantic correctness reward uses a cross-encoder model (cross-encoder/stsb-roberta-base) to evaluate the similarity between generated responses and reference answers. When no valid answer is extracted (i.e., an empty response), the function assigns a reward of -1.0 to indicate failure in producing an answer.\n",
    "\n",
    "#The perplexity calculation is handled by the PerplexityCalculator class which uses BioGPT (microsoft/biogpt) to measure the fluency and linguistic quality of responses. Note that BioGPT typically demands extra GPU resources due to its computational intensity. If a response is empty or an error occurs (e.g., a NaN loss), the function returns a default perplexity value of -1.0 to signal the issue.\n",
    "\n",
    "#The combined reward function aggregates the rewards from semantic correctness and perplexity. The rewards are weighted, with semantic similarity and perplexity contributing most significantly to form a final score. The final reward is clamped between -1 and 1, ensuring that any instance where no valid answer is found (or other issues arise) results in a -1 reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae9630b-e8d8-40eb-a143-6cf0d70e850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import CrossEncoder\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "# ------ Device Configuration ------\n",
    "main_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "reward_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ------ Semantic Correctness Reward ------\n",
    "def semantic_correctness(responses: List[str], answers: List[str]) -> List[float]:\n",
    "    \"\"\"Calculate semantic similarity using cross-encoder\"\"\"\n",
    "    model = CrossEncoder('cross-encoder/stsb-roberta-base', device=reward_device)\n",
    "    with torch.no_grad():            \n",
    "        inputs = list(zip(responses, answers))\n",
    "        similarities = model.predict(inputs, show_progress_bar=False).tolist()\n",
    "        # Set similarity to -1 if the response is an empty string\n",
    "        similarities = [-1.0 if response == \"\" else similarity for response, similarity in zip(responses, similarities)]\n",
    "        return similarities\n",
    "\n",
    "# ------ Efficient Perplexity Calculation ------\n",
    "class PerplexityCalculator:\n",
    "    def __init__(self, model_name=\"microsoft/biogpt\", device=reward_device):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.device = device\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def calculate(self, texts: List[str], batch_size=8) -> List[float]:\n",
    "        perplexities = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            try:\n",
    "                if not batch: continue\n",
    "                \n",
    "                encodings = self.tokenizer(\n",
    "                    batch, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True, \n",
    "                    truncation=True,\n",
    "                    max_length=200\n",
    "                ).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**encodings, labels=encodings.input_ids)\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                if torch.isnan(loss):\n",
    "                    raise ValueError(\"NaN loss encountered\")\n",
    "                \n",
    "                batch_perplexity = torch.exp(loss).repeat(len(batch)).cpu().tolist()\n",
    "                # Set perplexity to -1 if the input is an empty string\n",
    "                batch_perplexity = [-1.0 if text == \"\" else perplex for text, perplex in zip(batch, batch_perplexity)]\n",
    "                perplexities.extend(batch_perplexity)\n",
    "                                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//batch_size}: {str(e)}\")\n",
    "                perplexities.extend([1000.0] * len(batch))\n",
    "        \n",
    "        return perplexities\n",
    "\n",
    "perplexity_calculator = PerplexityCalculator()\n",
    "\n",
    "# ------ Tag Presence Reward ------\n",
    "def tag_presence_reward(completions: List[dict]) -> List[float]:\n",
    "    \"\"\"Reward for presence of <reasoning> and <answer> tags\"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        content = completion[0]['content']\n",
    "        has_reasoning = bool(re.search(r'<reasoning>.*?</reasoning>', content, re.DOTALL))\n",
    "        has_answer = bool(re.search(r'<answer>.*?</answer>', content, re.DOTALL))\n",
    "        reward = 0.5 * has_reasoning + 0.5 * has_answer\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "# ------ Combined Reward Function ------\n",
    "def combined_reward_func(\n",
    "    prompts, completions, answer, **kwargs\n",
    ") -> List[float]:\n",
    "    # Extract generated responses\n",
    "    responses = []\n",
    "    valid_indices = []\n",
    "    for idx, completion in enumerate(completions):\n",
    "        try:\n",
    "            generated_content = completion[0]['content'].strip()\n",
    "\n",
    "            # Extract only the <answer> content using regex\n",
    "            answer_match = re.search(r'<answer>(.*?)</answer>', generated_content, re.DOTALL)\n",
    "            if answer_match:\n",
    "                generated_content = answer_match.group(1).strip()\n",
    "            else:\n",
    "                responses.append(\"\")  # Append empty string to maintain index consistency\n",
    "                valid_indices.append(idx)\n",
    "                continue\n",
    "            \n",
    "            # Skip if empty or just repeating the prompt\n",
    "            user_prompt = prompts[idx][-1]['content']\n",
    "            if not generated_content or generated_content == user_prompt:\n",
    "                # print(f\"generated_content: continue\")\n",
    "                responses.append(\"\")  # Append empty string to maintain index consistency\n",
    "                valid_indices.append(idx)\n",
    "                continue\n",
    "                \n",
    "            responses.append(generated_content)\n",
    "            # print(f\"generated_content3: {generated_content}\")\n",
    "            valid_indices.append(idx)\n",
    "        except (KeyError, IndexError):\n",
    "            responses.append(\"\")  # Append empty string to maintain index consistency\n",
    "            valid_indices.append(idx)\n",
    "            continue\n",
    "    \n",
    "    if not responses:\n",
    "        return [-1.0] * len(completions)\n",
    "    \n",
    "    # Calculate rewards\n",
    "    try:\n",
    "        # Use raw answers without tag processing\n",
    "        processed_answers = answer\n",
    "        # print(f\"processed_answers: {processed_answers[0]}\")\n",
    "        \n",
    "        similarities = semantic_correctness(responses, [processed_answers[i] for i in valid_indices])\n",
    "        perplexities = perplexity_calculator.calculate([processed_answers[i] for i in valid_indices])\n",
    "        tag_rewards = tag_presence_reward([completions[i] for i in valid_indices])\n",
    "    except Exception as e:\n",
    "        print(f\"Reward calculation error: {str(e)}\")\n",
    "        return [-1.0] * len(completions)\n",
    "    \n",
    "    # Convert to tensors with stability\n",
    "    sim_scores = torch.nan_to_num(torch.tensor(similarities), nan=0.0)\n",
    "    perplex_scores = torch.nan_to_num(torch.tensor(perplexities), nan=1000.0)\n",
    "    tag_scores = torch.tensor(tag_rewards)\n",
    "        \n",
    "    # Perplexity reward calculation\n",
    "    perplex_rewards = 1 / (perplex_scores / (perplex_scores.mean() + 1e-9))\n",
    "    \n",
    "    # Normalize with stability\n",
    "    score_range = perplex_rewards.max() - perplex_rewards.min()\n",
    "    if score_range < 1e-6:\n",
    "        perplex_rewards_normalized = torch.ones_like(perplex_rewards) * 0.5\n",
    "    else:\n",
    "        perplex_rewards_normalized = (perplex_rewards - perplex_rewards.min()) / score_range\n",
    "    \n",
    "    # Combine scores with validation\n",
    "    combined = [\n",
    "        0.5 * sim.item() + 0.4 * pr.item() + 0.1 * tag.item()\n",
    "        for sim, pr, tag in zip(sim_scores, perplex_rewards_normalized, tag_scores)\n",
    "        if not torch.isnan(sim) and not torch.isnan(pr) and not torch.isnan(tag)\n",
    "    ]\n",
    "    \n",
    "    # Map back to original indices\n",
    "    final_rewards = [-1.0] * len(completions)\n",
    "    for idx, reward in zip(valid_indices, combined):\n",
    "        final_rewards[idx] = max(min(reward, 1.0), -1.0)  # Clamp between -1 and 1\n",
    "    \n",
    "    assert len(final_rewards) == len(completions), \"Reward mapping error\"\n",
    "    return final_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4760bebe-ac05-4648-90f5-41b5964e4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "#Now set up GRPO Trainer and all configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1498bb02-394d-4318-b023-7d3b36b55876",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 2 # 2 for quicker iterations, 4 otherwise\n",
    "\n",
    "total_steps = 200  # Changed from max steps of 1000\n",
    "num_checkpoints = 4\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm=True,  # use vLLM for fast inference!\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_generations=5,  # Decrease if out of memory\n",
    "    max_prompt_length=128,  # Updated by lowering to 128 from 512 to balance longer input prompts with training time requirements\n",
    "    max_completion_length=128,\n",
    "    max_steps=total_steps,\n",
    "    save_steps=int(total_steps // num_checkpoints),\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"none\",  # Can use Weights & Biases\n",
    "    output_dir=\"grpo_outputs\",\n",
    "    save_strategy=\"steps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6dd79e8-d099-4a70-b810-809995115e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's run the trainer. If you scroll, you'll see a table of rewards. The goal is to see the reward column increase.\n",
    "#You might have to wait 150 to 200 steps for any action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0575909-1ced-48e9-89af-9978454d207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        combined_reward_func\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a46eeff-1fc2-47f2-ba15-8e96a0496a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 25,117 | Num Epochs = 1 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 2 x 1) = 2\n",
      " \"-____-\"     Trainable parameters = 167,772,160/8,000,000,000 (2.10% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 3:16:13, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completion_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / combined_reward_func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.205303</td>\n",
       "      <td>0.211749</td>\n",
       "      <td>121.900002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.205303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>124.600002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.206189</td>\n",
       "      <td>0.196091</td>\n",
       "      <td>113.900002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.206189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167601</td>\n",
       "      <td>0.410170</td>\n",
       "      <td>112.900002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.167601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.209033</td>\n",
       "      <td>0.203408</td>\n",
       "      <td>113.000004</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.209033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.121850</td>\n",
       "      <td>0.243961</td>\n",
       "      <td>126.100002</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-0.121850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.205588</td>\n",
       "      <td>0.197433</td>\n",
       "      <td>123.600002</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.205588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>119.799999</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.015486</td>\n",
       "      <td>0.237122</td>\n",
       "      <td>108.099998</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.015486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.211745</td>\n",
       "      <td>0.197345</td>\n",
       "      <td>121.400002</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>-0.211745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.136868</td>\n",
       "      <td>0.323690</td>\n",
       "      <td>100.100002</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>-0.136868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>127.400002</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.285000</td>\n",
       "      <td>0.013693</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.280000</td>\n",
       "      <td>0.024873</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.106101</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>109.299999</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>-0.106101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.295000</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>-0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.141911</td>\n",
       "      <td>0.220992</td>\n",
       "      <td>103.500000</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>-0.141911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.204273</td>\n",
       "      <td>0.200369</td>\n",
       "      <td>122.900002</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>-0.204273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.295000</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>124.600002</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>-0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.209669</td>\n",
       "      <td>0.188342</td>\n",
       "      <td>122.799999</td>\n",
       "      <td>0.004712</td>\n",
       "      <td>-0.209669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.075778</td>\n",
       "      <td>0.462246</td>\n",
       "      <td>94.200001</td>\n",
       "      <td>0.008508</td>\n",
       "      <td>0.075778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-0.197021</td>\n",
       "      <td>0.216577</td>\n",
       "      <td>126.200001</td>\n",
       "      <td>0.011972</td>\n",
       "      <td>-0.197021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.205277</td>\n",
       "      <td>0.211807</td>\n",
       "      <td>126.600002</td>\n",
       "      <td>0.005367</td>\n",
       "      <td>-0.205277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>-0.295000</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.007697</td>\n",
       "      <td>-0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.066624</td>\n",
       "      <td>0.450074</td>\n",
       "      <td>120.900002</td>\n",
       "      <td>0.005483</td>\n",
       "      <td>0.066624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-0.290000</td>\n",
       "      <td>0.022361</td>\n",
       "      <td>121.500000</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>-0.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>-0.171155</td>\n",
       "      <td>0.226607</td>\n",
       "      <td>121.799999</td>\n",
       "      <td>0.022133</td>\n",
       "      <td>-0.171155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.201696</td>\n",
       "      <td>0.206123</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>-0.201696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>-0.188362</td>\n",
       "      <td>0.208002</td>\n",
       "      <td>105.900002</td>\n",
       "      <td>0.048348</td>\n",
       "      <td>-0.188362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-0.184100</td>\n",
       "      <td>0.198087</td>\n",
       "      <td>127.799999</td>\n",
       "      <td>0.014185</td>\n",
       "      <td>-0.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>-0.001314</td>\n",
       "      <td>0.428878</td>\n",
       "      <td>123.600002</td>\n",
       "      <td>0.025165</td>\n",
       "      <td>-0.001314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-0.050293</td>\n",
       "      <td>0.228079</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.015181</td>\n",
       "      <td>-0.050293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.513711</td>\n",
       "      <td>0.213582</td>\n",
       "      <td>111.500000</td>\n",
       "      <td>0.028430</td>\n",
       "      <td>0.513711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.185208</td>\n",
       "      <td>0.022404</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.039294</td>\n",
       "      <td>0.185208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>-0.108384</td>\n",
       "      <td>0.259351</td>\n",
       "      <td>104.500000</td>\n",
       "      <td>0.042824</td>\n",
       "      <td>-0.108384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>-0.012173</td>\n",
       "      <td>0.418939</td>\n",
       "      <td>122.799999</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>-0.012173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.251999</td>\n",
       "      <td>0.443363</td>\n",
       "      <td>117.600002</td>\n",
       "      <td>0.034506</td>\n",
       "      <td>0.251999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-0.109676</td>\n",
       "      <td>0.249410</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.014171</td>\n",
       "      <td>-0.109676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>-0.090194</td>\n",
       "      <td>0.264757</td>\n",
       "      <td>122.200001</td>\n",
       "      <td>0.016695</td>\n",
       "      <td>-0.090194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.062094</td>\n",
       "      <td>0.189220</td>\n",
       "      <td>124.600002</td>\n",
       "      <td>0.028775</td>\n",
       "      <td>0.062094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.327528</td>\n",
       "      <td>0.241430</td>\n",
       "      <td>90.600000</td>\n",
       "      <td>0.064392</td>\n",
       "      <td>0.327528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.322665</td>\n",
       "      <td>0.417593</td>\n",
       "      <td>116.600002</td>\n",
       "      <td>0.056148</td>\n",
       "      <td>0.322665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.162302</td>\n",
       "      <td>0.033724</td>\n",
       "      <td>117.800003</td>\n",
       "      <td>0.042442</td>\n",
       "      <td>0.162302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.081948</td>\n",
       "      <td>0.448361</td>\n",
       "      <td>126.400002</td>\n",
       "      <td>0.059159</td>\n",
       "      <td>0.081948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.058651</td>\n",
       "      <td>0.379209</td>\n",
       "      <td>117.900002</td>\n",
       "      <td>0.068238</td>\n",
       "      <td>0.058651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.076558</td>\n",
       "      <td>0.427696</td>\n",
       "      <td>126.900002</td>\n",
       "      <td>0.060004</td>\n",
       "      <td>0.076558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>-0.005911</td>\n",
       "      <td>0.222826</td>\n",
       "      <td>127.299999</td>\n",
       "      <td>0.032535</td>\n",
       "      <td>-0.005911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>-0.071228</td>\n",
       "      <td>0.256408</td>\n",
       "      <td>127.900002</td>\n",
       "      <td>0.043342</td>\n",
       "      <td>-0.071228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>-0.190910</td>\n",
       "      <td>0.221572</td>\n",
       "      <td>95.900000</td>\n",
       "      <td>0.051026</td>\n",
       "      <td>-0.190910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.163890</td>\n",
       "      <td>0.022517</td>\n",
       "      <td>103.799999</td>\n",
       "      <td>0.090559</td>\n",
       "      <td>0.163890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.192637</td>\n",
       "      <td>0.481690</td>\n",
       "      <td>98.800003</td>\n",
       "      <td>0.074080</td>\n",
       "      <td>0.192637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.618831</td>\n",
       "      <td>0.011419</td>\n",
       "      <td>112.299999</td>\n",
       "      <td>0.086059</td>\n",
       "      <td>0.618831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.417784</td>\n",
       "      <td>0.200073</td>\n",
       "      <td>115.400002</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.417784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.554259</td>\n",
       "      <td>0.192559</td>\n",
       "      <td>112.200001</td>\n",
       "      <td>0.054056</td>\n",
       "      <td>0.554259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.611373</td>\n",
       "      <td>0.039653</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.062864</td>\n",
       "      <td>0.611373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.226177</td>\n",
       "      <td>121.600002</td>\n",
       "      <td>0.076193</td>\n",
       "      <td>0.552326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.547338</td>\n",
       "      <td>0.201158</td>\n",
       "      <td>93.200001</td>\n",
       "      <td>0.070438</td>\n",
       "      <td>0.547338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.633206</td>\n",
       "      <td>0.018088</td>\n",
       "      <td>106.600002</td>\n",
       "      <td>0.086764</td>\n",
       "      <td>0.633206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.197920</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>105.299999</td>\n",
       "      <td>0.069517</td>\n",
       "      <td>0.197920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.503820</td>\n",
       "      <td>0.207790</td>\n",
       "      <td>126.600002</td>\n",
       "      <td>0.043496</td>\n",
       "      <td>0.503820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.172855</td>\n",
       "      <td>0.013693</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.095316</td>\n",
       "      <td>0.172855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.276158</td>\n",
       "      <td>0.480385</td>\n",
       "      <td>120.300003</td>\n",
       "      <td>0.073816</td>\n",
       "      <td>0.276158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>0.255373</td>\n",
       "      <td>115.700001</td>\n",
       "      <td>0.126837</td>\n",
       "      <td>0.403034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>-0.086368</td>\n",
       "      <td>0.224111</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.090871</td>\n",
       "      <td>-0.086368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.460160</td>\n",
       "      <td>0.230846</td>\n",
       "      <td>105.299999</td>\n",
       "      <td>0.051002</td>\n",
       "      <td>0.460160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.179338</td>\n",
       "      <td>0.391896</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.047859</td>\n",
       "      <td>0.179338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.274326</td>\n",
       "      <td>0.181299</td>\n",
       "      <td>121.100002</td>\n",
       "      <td>0.077156</td>\n",
       "      <td>0.274326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.641862</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>121.100002</td>\n",
       "      <td>0.069640</td>\n",
       "      <td>0.641862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.123232</td>\n",
       "      <td>0.208910</td>\n",
       "      <td>113.200001</td>\n",
       "      <td>0.095717</td>\n",
       "      <td>0.123232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.444509</td>\n",
       "      <td>0.263764</td>\n",
       "      <td>113.000004</td>\n",
       "      <td>0.092707</td>\n",
       "      <td>0.444509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.075435</td>\n",
       "      <td>0.197014</td>\n",
       "      <td>120.600002</td>\n",
       "      <td>0.092572</td>\n",
       "      <td>0.075435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.540928</td>\n",
       "      <td>0.221893</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.092953</td>\n",
       "      <td>0.540928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.536148</td>\n",
       "      <td>0.223040</td>\n",
       "      <td>115.300003</td>\n",
       "      <td>0.091055</td>\n",
       "      <td>0.536148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.401967</td>\n",
       "      <td>0.244695</td>\n",
       "      <td>111.500000</td>\n",
       "      <td>0.074403</td>\n",
       "      <td>0.401967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.421207</td>\n",
       "      <td>0.267601</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.231455</td>\n",
       "      <td>0.421207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.090514</td>\n",
       "      <td>0.210062</td>\n",
       "      <td>122.799999</td>\n",
       "      <td>0.125932</td>\n",
       "      <td>0.090514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.271063</td>\n",
       "      <td>0.193532</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.103539</td>\n",
       "      <td>0.271063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.431072</td>\n",
       "      <td>0.388837</td>\n",
       "      <td>115.700001</td>\n",
       "      <td>0.323561</td>\n",
       "      <td>0.431072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.407990</td>\n",
       "      <td>0.239106</td>\n",
       "      <td>124.299999</td>\n",
       "      <td>0.053353</td>\n",
       "      <td>0.407990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.417306</td>\n",
       "      <td>0.264315</td>\n",
       "      <td>107.100002</td>\n",
       "      <td>0.097610</td>\n",
       "      <td>0.417306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.443215</td>\n",
       "      <td>0.262193</td>\n",
       "      <td>118.299999</td>\n",
       "      <td>0.086669</td>\n",
       "      <td>0.443215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.147097</td>\n",
       "      <td>0.019348</td>\n",
       "      <td>127.200001</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.147097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.415093</td>\n",
       "      <td>0.171069</td>\n",
       "      <td>88.400000</td>\n",
       "      <td>0.059721</td>\n",
       "      <td>0.415093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.214923</td>\n",
       "      <td>0.019975</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.085497</td>\n",
       "      <td>0.214923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.563560</td>\n",
       "      <td>0.219568</td>\n",
       "      <td>117.900002</td>\n",
       "      <td>0.108553</td>\n",
       "      <td>0.563560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.091974</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>123.400002</td>\n",
       "      <td>0.098758</td>\n",
       "      <td>0.091974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>-0.009434</td>\n",
       "      <td>0.407735</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.078774</td>\n",
       "      <td>-0.009434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.240416</td>\n",
       "      <td>0.434561</td>\n",
       "      <td>127.700001</td>\n",
       "      <td>0.122804</td>\n",
       "      <td>0.240416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.506674</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>117.500000</td>\n",
       "      <td>0.104437</td>\n",
       "      <td>0.506674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.004367</td>\n",
       "      <td>0.232312</td>\n",
       "      <td>119.700001</td>\n",
       "      <td>0.114688</td>\n",
       "      <td>0.004367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.433975</td>\n",
       "      <td>0.382821</td>\n",
       "      <td>111.500004</td>\n",
       "      <td>0.127026</td>\n",
       "      <td>0.433975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.105792</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>127.799999</td>\n",
       "      <td>0.054037</td>\n",
       "      <td>0.105792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.531535</td>\n",
       "      <td>0.217701</td>\n",
       "      <td>105.500000</td>\n",
       "      <td>0.101380</td>\n",
       "      <td>0.531535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.571887</td>\n",
       "      <td>0.021428</td>\n",
       "      <td>124.900002</td>\n",
       "      <td>0.093070</td>\n",
       "      <td>0.571887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.588192</td>\n",
       "      <td>0.080443</td>\n",
       "      <td>104.299999</td>\n",
       "      <td>0.101825</td>\n",
       "      <td>0.588192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.272249</td>\n",
       "      <td>0.488715</td>\n",
       "      <td>120.300003</td>\n",
       "      <td>0.084641</td>\n",
       "      <td>0.272249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.509217</td>\n",
       "      <td>0.228152</td>\n",
       "      <td>102.400002</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>0.509217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.613805</td>\n",
       "      <td>0.006196</td>\n",
       "      <td>104.400002</td>\n",
       "      <td>0.068314</td>\n",
       "      <td>0.613805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.099894</td>\n",
       "      <td>0.350463</td>\n",
       "      <td>122.900002</td>\n",
       "      <td>0.125505</td>\n",
       "      <td>0.099894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.604488</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.063143</td>\n",
       "      <td>0.604488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.142303</td>\n",
       "      <td>0.426449</td>\n",
       "      <td>125.200001</td>\n",
       "      <td>0.060515</td>\n",
       "      <td>0.142303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.503934</td>\n",
       "      <td>0.252202</td>\n",
       "      <td>108.700005</td>\n",
       "      <td>0.105074</td>\n",
       "      <td>0.503934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.343693</td>\n",
       "      <td>0.262427</td>\n",
       "      <td>125.900002</td>\n",
       "      <td>0.093082</td>\n",
       "      <td>0.343693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.600357</td>\n",
       "      <td>0.024509</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>0.060687</td>\n",
       "      <td>0.600357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.062115</td>\n",
       "      <td>0.174509</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.062115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.575705</td>\n",
       "      <td>0.031234</td>\n",
       "      <td>122.600002</td>\n",
       "      <td>0.087254</td>\n",
       "      <td>0.575705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.186846</td>\n",
       "      <td>0.025896</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>0.184151</td>\n",
       "      <td>0.186846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.459627</td>\n",
       "      <td>0.214746</td>\n",
       "      <td>108.600002</td>\n",
       "      <td>0.082768</td>\n",
       "      <td>0.459627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.144799</td>\n",
       "      <td>0.052440</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.098688</td>\n",
       "      <td>0.144799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.114443</td>\n",
       "      <td>0.403569</td>\n",
       "      <td>121.400002</td>\n",
       "      <td>0.130306</td>\n",
       "      <td>0.114443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.424185</td>\n",
       "      <td>0.364208</td>\n",
       "      <td>122.700001</td>\n",
       "      <td>0.086628</td>\n",
       "      <td>0.424185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.553803</td>\n",
       "      <td>0.202883</td>\n",
       "      <td>117.799999</td>\n",
       "      <td>0.126507</td>\n",
       "      <td>0.553803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.637066</td>\n",
       "      <td>0.031383</td>\n",
       "      <td>103.400002</td>\n",
       "      <td>0.137769</td>\n",
       "      <td>0.637066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.326315</td>\n",
       "      <td>0.280009</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.109018</td>\n",
       "      <td>0.326315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.576690</td>\n",
       "      <td>0.094815</td>\n",
       "      <td>98.600002</td>\n",
       "      <td>0.103299</td>\n",
       "      <td>0.576690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.511386</td>\n",
       "      <td>0.201421</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.103917</td>\n",
       "      <td>0.511386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.276310</td>\n",
       "      <td>0.193784</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.093899</td>\n",
       "      <td>0.276310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.379989</td>\n",
       "      <td>0.247326</td>\n",
       "      <td>98.799999</td>\n",
       "      <td>0.137274</td>\n",
       "      <td>0.379989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.276684</td>\n",
       "      <td>0.430745</td>\n",
       "      <td>125.100002</td>\n",
       "      <td>0.093803</td>\n",
       "      <td>0.276684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.173516</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.098348</td>\n",
       "      <td>0.173516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.331866</td>\n",
       "      <td>0.210415</td>\n",
       "      <td>122.100002</td>\n",
       "      <td>0.065827</td>\n",
       "      <td>0.331866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.421575</td>\n",
       "      <td>0.221024</td>\n",
       "      <td>102.799999</td>\n",
       "      <td>0.211874</td>\n",
       "      <td>0.421575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.399388</td>\n",
       "      <td>0.256855</td>\n",
       "      <td>85.100000</td>\n",
       "      <td>0.063903</td>\n",
       "      <td>0.399388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.585286</td>\n",
       "      <td>0.039176</td>\n",
       "      <td>120.299999</td>\n",
       "      <td>0.072511</td>\n",
       "      <td>0.585286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.198668</td>\n",
       "      <td>0.491437</td>\n",
       "      <td>125.299999</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.198668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.576243</td>\n",
       "      <td>0.008411</td>\n",
       "      <td>94.200003</td>\n",
       "      <td>0.069426</td>\n",
       "      <td>0.576243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.491837</td>\n",
       "      <td>0.229037</td>\n",
       "      <td>119.300003</td>\n",
       "      <td>0.087691</td>\n",
       "      <td>0.491837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.578572</td>\n",
       "      <td>0.114545</td>\n",
       "      <td>121.900002</td>\n",
       "      <td>0.091916</td>\n",
       "      <td>0.578572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.064993</td>\n",
       "      <td>0.584337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.330835</td>\n",
       "      <td>0.201034</td>\n",
       "      <td>122.900002</td>\n",
       "      <td>0.091784</td>\n",
       "      <td>0.330835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.610546</td>\n",
       "      <td>0.026130</td>\n",
       "      <td>120.900002</td>\n",
       "      <td>0.101829</td>\n",
       "      <td>0.610546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.639707</td>\n",
       "      <td>0.022218</td>\n",
       "      <td>97.100002</td>\n",
       "      <td>0.146386</td>\n",
       "      <td>0.639707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.382645</td>\n",
       "      <td>0.442745</td>\n",
       "      <td>119.200001</td>\n",
       "      <td>0.120491</td>\n",
       "      <td>0.382645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>-0.019979</td>\n",
       "      <td>0.221668</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.068377</td>\n",
       "      <td>-0.019979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.511850</td>\n",
       "      <td>0.197352</td>\n",
       "      <td>107.800003</td>\n",
       "      <td>0.059095</td>\n",
       "      <td>0.511850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.271813</td>\n",
       "      <td>0.173116</td>\n",
       "      <td>127.600002</td>\n",
       "      <td>0.123718</td>\n",
       "      <td>0.271813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.264211</td>\n",
       "      <td>0.296136</td>\n",
       "      <td>87.900000</td>\n",
       "      <td>0.210475</td>\n",
       "      <td>0.264211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.621355</td>\n",
       "      <td>0.030896</td>\n",
       "      <td>110.600002</td>\n",
       "      <td>0.094119</td>\n",
       "      <td>0.621355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.596523</td>\n",
       "      <td>0.030812</td>\n",
       "      <td>117.900002</td>\n",
       "      <td>0.068421</td>\n",
       "      <td>0.596523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.466596</td>\n",
       "      <td>0.256778</td>\n",
       "      <td>124.900002</td>\n",
       "      <td>0.061427</td>\n",
       "      <td>0.466596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.333606</td>\n",
       "      <td>0.240936</td>\n",
       "      <td>118.600002</td>\n",
       "      <td>0.100415</td>\n",
       "      <td>0.333606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.285762</td>\n",
       "      <td>0.199062</td>\n",
       "      <td>111.600002</td>\n",
       "      <td>0.098408</td>\n",
       "      <td>0.285762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.598785</td>\n",
       "      <td>0.043527</td>\n",
       "      <td>111.500004</td>\n",
       "      <td>0.117505</td>\n",
       "      <td>0.598785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.535536</td>\n",
       "      <td>0.186157</td>\n",
       "      <td>115.500000</td>\n",
       "      <td>0.068839</td>\n",
       "      <td>0.535536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.562599</td>\n",
       "      <td>0.216656</td>\n",
       "      <td>118.299999</td>\n",
       "      <td>0.077220</td>\n",
       "      <td>0.562599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.430460</td>\n",
       "      <td>0.226333</td>\n",
       "      <td>105.400002</td>\n",
       "      <td>0.076377</td>\n",
       "      <td>0.430460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.623548</td>\n",
       "      <td>0.033617</td>\n",
       "      <td>126.600002</td>\n",
       "      <td>0.063770</td>\n",
       "      <td>0.623548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.506819</td>\n",
       "      <td>0.228832</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.078811</td>\n",
       "      <td>0.506819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.670332</td>\n",
       "      <td>0.044315</td>\n",
       "      <td>82.000004</td>\n",
       "      <td>0.080469</td>\n",
       "      <td>0.670332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.183166</td>\n",
       "      <td>0.399460</td>\n",
       "      <td>125.400002</td>\n",
       "      <td>0.097340</td>\n",
       "      <td>0.183166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.199693</td>\n",
       "      <td>0.403554</td>\n",
       "      <td>119.100002</td>\n",
       "      <td>0.127147</td>\n",
       "      <td>0.199693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.436403</td>\n",
       "      <td>0.247418</td>\n",
       "      <td>96.700001</td>\n",
       "      <td>0.101104</td>\n",
       "      <td>0.436403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.667792</td>\n",
       "      <td>0.030365</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.097859</td>\n",
       "      <td>0.667792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.547948</td>\n",
       "      <td>0.198726</td>\n",
       "      <td>104.900002</td>\n",
       "      <td>0.112564</td>\n",
       "      <td>0.547948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.593115</td>\n",
       "      <td>0.010408</td>\n",
       "      <td>127.799999</td>\n",
       "      <td>0.083083</td>\n",
       "      <td>0.593115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.277817</td>\n",
       "      <td>0.207940</td>\n",
       "      <td>108.200001</td>\n",
       "      <td>0.127174</td>\n",
       "      <td>0.277817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.578154</td>\n",
       "      <td>0.019353</td>\n",
       "      <td>98.900002</td>\n",
       "      <td>0.094757</td>\n",
       "      <td>0.578154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.432801</td>\n",
       "      <td>0.255802</td>\n",
       "      <td>97.100002</td>\n",
       "      <td>0.171878</td>\n",
       "      <td>0.432801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.537149</td>\n",
       "      <td>0.213836</td>\n",
       "      <td>115.400002</td>\n",
       "      <td>0.088825</td>\n",
       "      <td>0.537149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>0.346972</td>\n",
       "      <td>0.267568</td>\n",
       "      <td>117.500004</td>\n",
       "      <td>2.761441</td>\n",
       "      <td>0.346972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.473320</td>\n",
       "      <td>0.282778</td>\n",
       "      <td>109.299999</td>\n",
       "      <td>0.070591</td>\n",
       "      <td>0.473320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.521368</td>\n",
       "      <td>0.208816</td>\n",
       "      <td>124.500000</td>\n",
       "      <td>0.162093</td>\n",
       "      <td>0.521368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.062470</td>\n",
       "      <td>0.431203</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>0.097085</td>\n",
       "      <td>0.062470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.375090</td>\n",
       "      <td>0.256325</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>0.107908</td>\n",
       "      <td>0.375090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.522246</td>\n",
       "      <td>0.213303</td>\n",
       "      <td>127.500000</td>\n",
       "      <td>0.077240</td>\n",
       "      <td>0.522246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.361369</td>\n",
       "      <td>0.421307</td>\n",
       "      <td>118.300003</td>\n",
       "      <td>0.105961</td>\n",
       "      <td>0.361369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.415992</td>\n",
       "      <td>0.269246</td>\n",
       "      <td>87.400002</td>\n",
       "      <td>0.082619</td>\n",
       "      <td>0.415992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.294682</td>\n",
       "      <td>0.283677</td>\n",
       "      <td>125.100002</td>\n",
       "      <td>0.158079</td>\n",
       "      <td>0.294682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.352884</td>\n",
       "      <td>0.255991</td>\n",
       "      <td>121.500004</td>\n",
       "      <td>0.072731</td>\n",
       "      <td>0.352884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.267907</td>\n",
       "      <td>0.486241</td>\n",
       "      <td>110.500004</td>\n",
       "      <td>0.219594</td>\n",
       "      <td>0.267907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.588516</td>\n",
       "      <td>0.041757</td>\n",
       "      <td>119.100002</td>\n",
       "      <td>0.128591</td>\n",
       "      <td>0.588516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.506273</td>\n",
       "      <td>0.216834</td>\n",
       "      <td>110.800003</td>\n",
       "      <td>0.284742</td>\n",
       "      <td>0.506273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.448544</td>\n",
       "      <td>0.240074</td>\n",
       "      <td>113.600002</td>\n",
       "      <td>0.075371</td>\n",
       "      <td>0.448544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.162906</td>\n",
       "      <td>0.238489</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.064389</td>\n",
       "      <td>0.162906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.628914</td>\n",
       "      <td>0.018052</td>\n",
       "      <td>101.100002</td>\n",
       "      <td>0.117567</td>\n",
       "      <td>0.628914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.429238</td>\n",
       "      <td>0.380087</td>\n",
       "      <td>120.299999</td>\n",
       "      <td>0.140856</td>\n",
       "      <td>0.429238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.599026</td>\n",
       "      <td>0.026942</td>\n",
       "      <td>93.100002</td>\n",
       "      <td>0.160385</td>\n",
       "      <td>0.599026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>-0.030044</td>\n",
       "      <td>0.230242</td>\n",
       "      <td>126.100002</td>\n",
       "      <td>0.113395</td>\n",
       "      <td>-0.030044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.374850</td>\n",
       "      <td>0.243901</td>\n",
       "      <td>116.900002</td>\n",
       "      <td>0.088680</td>\n",
       "      <td>0.374850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.570244</td>\n",
       "      <td>0.039448</td>\n",
       "      <td>110.700001</td>\n",
       "      <td>0.132932</td>\n",
       "      <td>0.570244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.554849</td>\n",
       "      <td>0.029338</td>\n",
       "      <td>92.200001</td>\n",
       "      <td>0.082356</td>\n",
       "      <td>0.554849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.504315</td>\n",
       "      <td>0.222571</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.099873</td>\n",
       "      <td>0.504315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.076085</td>\n",
       "      <td>0.206845</td>\n",
       "      <td>125.600002</td>\n",
       "      <td>0.104854</td>\n",
       "      <td>0.076085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.504021</td>\n",
       "      <td>0.185124</td>\n",
       "      <td>110.799999</td>\n",
       "      <td>0.088475</td>\n",
       "      <td>0.504021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.648369</td>\n",
       "      <td>0.021819</td>\n",
       "      <td>114.400002</td>\n",
       "      <td>0.184216</td>\n",
       "      <td>0.648369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.520255</td>\n",
       "      <td>0.191084</td>\n",
       "      <td>112.400002</td>\n",
       "      <td>0.095207</td>\n",
       "      <td>0.520255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.524657</td>\n",
       "      <td>0.222910</td>\n",
       "      <td>121.700001</td>\n",
       "      <td>0.056359</td>\n",
       "      <td>0.524657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.618838</td>\n",
       "      <td>0.035583</td>\n",
       "      <td>119.900002</td>\n",
       "      <td>0.130519</td>\n",
       "      <td>0.618838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>-0.001053</td>\n",
       "      <td>0.416618</td>\n",
       "      <td>122.799999</td>\n",
       "      <td>0.069136</td>\n",
       "      <td>-0.001053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.192707</td>\n",
       "      <td>0.508925</td>\n",
       "      <td>125.700005</td>\n",
       "      <td>0.180419</td>\n",
       "      <td>0.192707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.571347</td>\n",
       "      <td>0.033743</td>\n",
       "      <td>116.800003</td>\n",
       "      <td>0.074221</td>\n",
       "      <td>0.571347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.446418</td>\n",
       "      <td>0.403783</td>\n",
       "      <td>114.200001</td>\n",
       "      <td>0.131499</td>\n",
       "      <td>0.446418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.455608</td>\n",
       "      <td>0.246828</td>\n",
       "      <td>125.200001</td>\n",
       "      <td>0.117190</td>\n",
       "      <td>0.455608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>-0.177071</td>\n",
       "      <td>0.163073</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.054522</td>\n",
       "      <td>-0.177071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.0036339496986677756, metrics={'train_runtime': 11860.4504, 'train_samples_per_second': 0.034, 'train_steps_per_second': 0.017, 'total_flos': 0.0, 'train_loss': 0.0036339496986677756})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7275340d-4992-436b-853c-176fd75a3e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "#Now let's try the model we just trained: First, let's first try the model without any GRPO trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3aa44e9c-83dd-4324-ae6b-60026fd656a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:24<00:00, 24.88s/it, est. speed input: 1.85 t\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"Is Aspirin good for cardio vascular function?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1561145b-d518-4c2c-814f-86ea68b76ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aspirin's effect on cardiovascular function is complex and multifaceted. In small doses, aspirin is often recommended for people at risk of heart disease or those who have already had a heart attack or stroke. It acts as an antiplatelet agent, which helps prevent blood clots from forming and reducing the risk of heart attacks and strokes.\\n\\nHowever, it is essential to note that aspirin is not suitable for everyone, particularly those with a history of stomach ulcers or bleeding disorders. It's also not recommended for healthy individuals, as the risks may outweigh the benefits.\\n\\nAspirin's benefits for cardiovascular function include:\\n\\n1. **Antiplatelet activity**: Aspirin inhibits the production of thromboxane A2, a compound that promotes blood clotting, thus reducing the risk of blood clots forming.\\n2. **Inflammation reduction**: Aspirin has anti-inflammatory properties, which can help reduce inflammation in blood vessels, a risk factor for cardiovascular disease.\\n3. **Blood vessel dilation**: Aspirin can cause blood vessels to relax, improving blood flow and lowering blood pressure.\\n\\nHowever, there are also potential drawbacks to consider:\\n\\n1. **Gastrointestinal side effects**: Long-term aspirin use can lead to stomach ulcers, bleeding, and other gastrointestinal issues.\\n2. **Bleeding risk**: Aspirin can increase the risk of bleeding, particularly in older adults or those with bleeding disorders.\\n3. **Overuse**: Taking too much aspirin can lead to excessive bleeding, which can be severe and even life-threatening.\\n\\nTo determine whether aspirin is suitable for you, consult with your doctor or healthcare provider, who will assess your individual risk factors and medical history. They will help you decide whether the benefits of aspirin outweigh the risks.\\n\\n**Who should consider taking aspirin for cardiovascular health?**\\n\\n* People with a history of heart disease, stroke, or transient ischemic attack (TIA)\\n* Those with a family history of cardiovascular disease\\n* Individuals with diabetes, high blood pressure, or high cholesterol\\n* People with peripheral artery disease (PAD) or carotid artery disease\\n\\n**Who should not take aspirin?**\\n\\n* Those with a history of stomach ulcers or bleeding disorders\\n* Older adults with a history of falls or other bleeding risks\\n* Pregnant or breastfeeding women\\n* Individuals with kidney or liver disease\\n* People taking anticoagulant or antiplatelet medications without a valid medical reason\\n\\nIt is crucial to consult with your healthcare provider to weigh the benefits and risks of aspirin for your specific situation.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4bd88c4-7f38-470b-b6f2-beff1c4e5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#And now with the LoRA we just trained with GRPO - we save the LoRA first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d74213db-888f-436c-a7b7-4d70a0f87fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we load the LoRA and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db665710-8829-463f-87ec-94b8ff86d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c350c8b1-fd57-4da0-ab61-1debba92083a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:26<00:00, 26.60s/it, est. speed input: 2.56 t\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : \"Is Aspirin good for cardio vascular function?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46804d00-51c1-4e26-bea2-b990a01a97af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<reasoning>\\nAspirin has been a topic of interest in the context of cardiovascular health for many years.\\n</reasoning>\\n<answer>\\nAspirin may have both positive and negative effects on cardiovascular function, depending on the individual and their specific health status.\\n</answer>\\n\\nPositive effects:\\n\\n1. **Anti-clotting properties**: Aspirin's antiplatelet effect can help prevent blood clots from forming, reducing the risk of heart attacks and strokes in people with a history of cardiovascular disease.\\n2. **Reducing inflammation**: Aspirin has anti-inflammatory properties, which may help lower the risk of cardiovascular disease by reducing inflammation in the blood vessels.\\n3. **Lowering blood pressure**: Regular, low-dose aspirin use may help lower blood pressure in people with hypertension.\\n\\nNegative effects:\\n\\n1. **Bleeding risks**: Aspirin can increase the risk of bleeding, especially in people with a history of gastrointestinal problems or taking other medications that affect blood clotting.\\n2. **Gastrointestinal side effects**: Long-term aspirin use can cause stomach ulcers, bleeding in the stomach or intestines, and other gastrointestinal problems.\\n3. **Interactions with other medications**: Aspirin can interact with other medications, such as blood thinners, and increase the risk of bleeding or other complications.\\n\\n**Who should take aspirin for cardiovascular health?**\\n\\nAspirin may be recommended for:\\n\\n1. **People with a history of cardiovascular disease**: Those who have had a heart attack, stroke, or other cardiovascular event may benefit from aspirin's anti-clotting properties.\\n2. **People at high risk of cardiovascular disease**: Individuals with risk factors such as high blood pressure, high cholesterol, or diabetes may benefit from aspirin's anti-inflammatory and anti-clotting effects.\\n3. **People with atrial fibrillation**: Aspirin may help reduce the risk of stroke in people with atrial fibrillation.\\n\\n**Important notes**\\n\\n1. **Talk to your doctor**: Before starting aspirin therapy, discuss your individual risks and benefits with your healthcare provider.\\n2. **Low-dose aspirin**: Take low-dose aspirin (81 mg or 100 mg per day) to minimize bleeding risks.\\n3. **Regular monitoring**: Regularly check your blood pressure, kidney function, and liver function while taking aspirin.\\n\\nIn conclusion, aspirin can be beneficial for cardiovascular health in certain individuals, but it's essential to weigh the benefits against the potential risks and consult with a healthcare professional to determine the best course of action.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79be4f0-d542-4740-8cfb-4b802de9b412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 7.55 out of 30.89 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████▏                                       | 3/32 [00:00<00:02, 11.16it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|███████████████████████████████████████████| 32/32 [01:58<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"model\", tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
