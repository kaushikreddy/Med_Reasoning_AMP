{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19d7921",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Fine-Tuning with GRPO for Medical Reasoning - Example Notebook\n",
    "This notebook demonstrates how to fine-tune a language model using GRPO (Group Relative Policy Optimization) specifically for medical reasoning tasks. By leveraging custom reward functions and a specialized medical reasoning dataset, we transform a general-purpose language model into a domain-specific medical reasoning model.\n",
    "\n",
    "- Installation\n",
    "- Prepare data for GRPO training\n",
    "- Define custom reward functions to guide the modelâ€™s learning\n",
    "- Train a model using GRPO\n",
    "- Test the fine-tuned model\n",
    "- Save the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b41ac40",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e8683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install base requirements\n",
    "!pip install -r /home/cdsw/0_session-install-dependencies/requirements.txt && pip install git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b\n",
    "\n",
    "# Upgrade Unsloth packages\n",
    "!pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n",
    "\n",
    "# Verify installations\n",
    "import torch\n",
    "import transformers\n",
    "import unsloth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c7ddf69-b636-4ee6-9338-935dd5f35934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a378d788-0a87-432a-9cfe-e018fae99578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-24 15:09:42 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 15:09:45,993\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cce93f",
   "metadata": {},
   "source": [
    "## Load up `Llama 3.1 8B Instruct`, and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adbd2b03-9bbb-410e-a0a4-4d3a07f27b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.50.0. vLLM: 0.7.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit with actual GPU utilization = 49.53%\n",
      "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 0.97 GB. Also swap space = 5 GB.\n",
      "WARNING 03-24 15:09:57 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-24 15:10:16 config.py:542] This model supports multiple tasks: {'score', 'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 03-24 15:10:18 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=False, \n",
      "INFO 03-24 15:10:19 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-24 15:10:19 cuda.py:227] Using XFormers backend.\n",
      "INFO 03-24 15:10:20 model_runner.py:1110] Starting to load model unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W324 15:10:20.803733361 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 15:10:20 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 03-24 15:10:21 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.38s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.38s/it]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.05s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.05s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 15:10:38 model_runner.py:1115] Loading model weights took 5.5976 GB\n",
      "INFO 03-24 15:10:38 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 03-24 15:10:43 worker.py:267] Memory profiling takes 4.59 seconds\n",
      "INFO 03-24 15:10:43 worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.50) = 7.30GiB\n",
      "INFO 03-24 15:10:43 worker.py:267] model weights take 5.60GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.60GiB; the rest of the memory reserved for KV Cache is 1.08GiB.\n",
      "INFO 03-24 15:10:44 executor_base.py:110] # CUDA blocks: 551, # CPU blocks: 2560\n",
      "INFO 03-24 15:10:44 executor_base.py:115] Maximum concurrency for 1024 tokens per request: 8.61x\n",
      "INFO 03-24 15:10:48 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:21<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 15:11:10 model_runner.py:1562] Graph capturing finished in 22 secs, took 0.52 GiB\n",
      "INFO 03-24 15:11:10 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 31.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.3.18 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
    "lora_rank = 64 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.5, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # Remove QKVO if out of memory\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67417c4c",
   "metadata": {},
   "source": [
    "## Data Prep Strategy for GRPO training\n",
    "### Exploratory Data Analysis\n",
    "\n",
    "- Do basic EDA\n",
    "- Training, evaluation & testing split for reward modeling\n",
    "- Consistent random state (42) for reproducibility\n",
    "\n",
    "### Data Transformation\n",
    "\n",
    "- Adds a structured system prompt\n",
    "- Enforces a specific response format\n",
    "- Separates reasoning from answer\n",
    "- Prepares data for advanced medical reasoning models\n",
    "\n",
    "### Prompt Engineering\n",
    "\n",
    "- System prompt guides model's response structure\n",
    "- Encourages step-by-step reasoning\n",
    "- Separates reasoning from final answer\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "- Removes unnecessary columns\n",
    "- Standardizes dataset structure\n",
    "- Focuses on essential information (prompt, answer, question)\n",
    "\n",
    "### Data Prep for Reward Functions\n",
    "Data prep for supporting use-case-specific training data and setting up reward functions. For data prep and all reward functions, we leverage an open-source dataset - FreedomIntelligence/medical-o1-reasoning-SFT, a dataset used to fine-tune HuatuoGPT-o1, a medical LLM designed for advanced medical reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd  # Needed for EDA\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def get_medical_questions(split=\"train\", do_eda=False) -> Dataset:\n",
    "    # Load the raw dataset from Hugging Face\n",
    "    dataset = load_dataset('FreedomIntelligence/medical-o1-reasoning-SFT', 'en')\n",
    "    data = dataset[split]\n",
    "\n",
    "    # Convert to a pandas DataFrame for easier manipulation\n",
    "    df = data.to_pandas()\n",
    "\n",
    "    # Optional EDA\n",
    "    if do_eda:\n",
    "        print(\"\\n=== EDA: Previewing First 5 Rows ===\")\n",
    "        print(df.head())\n",
    "        print(\"\\n=== DataFrame Info ===\")\n",
    "        print(df.info())\n",
    "        print(\"\\n=== Null Values ===\")\n",
    "        print(df.isnull().sum())\n",
    "\n",
    "    if split == \"train\":\n",
    "        # Split into 99% train and 1% temp (for eval and test)\n",
    "        train_df, temp_df = train_test_split(df, test_size=0.01, random_state=42)\n",
    "        # Split 1% temp equally into eval and test\n",
    "        eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "        # Convert to HF datasets\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        eval_dataset = Dataset.from_pandas(eval_df)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "        def map_fn(x):\n",
    "            return {\n",
    "                'prompt': [\n",
    "                    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                    {'role': 'user', 'content': x['Question']}\n",
    "                ],\n",
    "                'answer': x['Response'],\n",
    "                'question': x['Question']\n",
    "            }\n",
    "\n",
    "        # Map and clean\n",
    "        train_dataset = train_dataset.map(map_fn).remove_columns(['Question', 'Complex_CoT', 'Response', '__index_level_0__'])\n",
    "        eval_dataset = eval_dataset.map(map_fn).remove_columns(['Question', 'Complex_CoT', 'Response', '__index_level_0__'])\n",
    "        test_dataset = test_dataset.map(map_fn).remove_columns(['Question', 'Complex_CoT', 'Response', '__index_level_0__'])\n",
    "\n",
    "        return train_dataset, eval_dataset, test_dataset\n",
    "\n",
    "    else:\n",
    "        # For eval/test only splits\n",
    "        data = data.map(lambda x: {\n",
    "            'prompt': [\n",
    "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'content': x['Question']}\n",
    "            ],\n",
    "            'answer': x['Response'],\n",
    "            'question': x['Question']\n",
    "        }).remove_columns(['Question', 'Complex_CoT', 'Response', '__index_level_0__'])\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b374f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with EDA enabled\n",
    "train_dataset, eval_dataset, test_dataset = get_medical_questions(split=\"train\", do_eda=True)\n",
    "\n",
    "# Check one example\n",
    "print(\"\\nSample from training dataset:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbb75c9",
   "metadata": {},
   "source": [
    "## Reward Functions & Reward Calculation Process\n",
    "\n",
    "### A. Response Extraction\n",
    "- Filters and processes generated responses\n",
    "- Handles edge cases:\n",
    "  - Empty responses\n",
    "  - Responses repeating the prompt\n",
    "  - Extracting `<answer>` content\n",
    "\n",
    "### B. Reward Calculation Steps\n",
    "- **Semantic Similarity Calculation**\n",
    "  - Uses `semantic_correctness()`\n",
    "  - Compares generated answers with ground truth\n",
    "- **Perplexity Assessment**\n",
    "  - Uses `PerplexityCalculator`\n",
    "  - Evaluates linguistic fluency\n",
    "- **Tag Presence Evaluation**\n",
    "  - Checks response structure compliance\n",
    "\n",
    "### C. Score Combination\n",
    "- Weighted combination:\n",
    "  - 50% Semantic Similarity\n",
    "  - 40% Perplexity Reward\n",
    "  - 10% Tag Presence\n",
    "\n",
    "### D. Normalization & Stabilization\n",
    "- Handles computational edge cases\n",
    "- Normalizes perplexity scores\n",
    "- Clamps final rewards between -1 and 1\n",
    "\n",
    "### E. Reward Interpretation\n",
    "- -1.0: Complete failure\n",
    "- 0.0: Neutral performance\n",
    "- 1.0: Perfect response\n",
    "\n",
    "## Reward Functions\n",
    "\n",
    "The semantic correctness reward uses a cross-encoder model (cross-encoder/stsb-roberta-base) to evaluate the similarity between generated responses and reference answers. When no valid answer is extracted (i.e., an empty response), the function assigns a reward of -1.0 to indicate failure in producing an answer.\n",
    "\n",
    "The perplexity calculation is handled by the `PerplexityCalculator` class which uses BioGPT (microsoft/biogpt) to measure the fluency and linguistic quality of responses. Note that BioGPT typically demands extra GPU resources due to its computational intensity. If a response is empty or an error occurs (e.g., a NaN loss), the function returns a default perplexity value of -1.0 to signal the issue.\n",
    "\n",
    "The combined reward function aggregates the rewards from semantic correctness and perplexity. The rewards are weighted, with semantic similarity and perplexity contributing most significantly to form a final score. The final reward is clamped between -1 and 1, ensuring that any instance where no valid answer is found (or other issues arise) results in a -1 reward.\n",
    "\n",
    "A Guide to Terms Used in This Notebook\n",
    "======================================\n",
    "\n",
    "| Term | Meaning in this Notebook Context |\n",
    "| --- | --- |\n",
    "| `prompt` | A list of messages, typically including:<br>`{'role': 'system', 'content': SYSTEM_PROMPT}`<br>`{'role': 'user', 'content': Question}` |\n",
    "| `completion` | A list of messages returned by the model. Each message is typically:<br>`{'role': 'assistant', 'content': ...}`<br>The `content` contains a structured answer with `<reasoning>` and `<answer>` tags. |\n",
    "| `answer` | The reference / ground truth answer from the dataset (human-written), in the same `<reasoning>...</reasoning><answer>...</answer>` format. |\n",
    "| `response` | The model's generated answer **after extraction** of the `<answer>` section only (used to compare with `answer`). |\n",
    "\n",
    "Processing Flow\n",
    "---------------\n",
    "\n",
    "1.  The model receives a medical question with instructions to provide reasoning and an answer\n",
    "2.  The model generates a completion with reasoning and answer in specified tags\n",
    "3.  The reward functions:\n",
    "    -   Extract the answer portion from the completion\n",
    "    -   Compare it to the reference answer for semantic similarity\n",
    "    -   Check the perplexity of the reference answer\n",
    "    -   Verify that proper tags are used\n",
    "4.  The combined rewards guide the GRPO process to improve model outputs\n",
    "\n",
    "This reward system balances accuracy (semantic similarity), fluency (perplexity), and format adherence (tag presence) to train the model to provide well-structured, accurate responses to medical reasoning questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae9630b-e8d8-40eb-a143-6cf0d70e850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import CrossEncoder\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "# ------ Device Configuration ------\n",
    "main_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "reward_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ------ Semantic Correctness Reward ------\n",
    "def semantic_correctness(responses: List[str], answers: List[str]) -> List[float]:\n",
    "    \"\"\"Calculate semantic similarity using cross-encoder\"\"\"\n",
    "    model = CrossEncoder('cross-encoder/stsb-roberta-base', device=reward_device)\n",
    "    with torch.no_grad():            \n",
    "        inputs = list(zip(responses, answers))\n",
    "        similarities = model.predict(inputs, show_progress_bar=False).tolist()\n",
    "        # Set similarity to -1 if the response is an empty string\n",
    "        similarities = [-1.0 if response == \"\" else similarity for response, similarity in zip(responses, similarities)]\n",
    "        return similarities\n",
    "\n",
    "# ------ Efficient Perplexity Calculation ------\n",
    "class PerplexityCalculator:\n",
    "    def __init__(self, model_name=\"microsoft/biogpt\", device=reward_device):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.device = device\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def calculate(self, texts: List[str], batch_size=8) -> List[float]:\n",
    "        perplexities = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            try:\n",
    "                if not batch: continue\n",
    "                \n",
    "                encodings = self.tokenizer(\n",
    "                    batch, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True, \n",
    "                    truncation=True,\n",
    "                    max_length=200\n",
    "                ).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**encodings, labels=encodings.input_ids)\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                if torch.isnan(loss):\n",
    "                    raise ValueError(\"NaN loss encountered\")\n",
    "                \n",
    "                batch_perplexity = torch.exp(loss).repeat(len(batch)).cpu().tolist()\n",
    "                # Set perplexity to -1 if the input is an empty string\n",
    "                batch_perplexity = [-1.0 if text == \"\" else perplex for text, perplex in zip(batch, batch_perplexity)]\n",
    "                perplexities.extend(batch_perplexity)\n",
    "                                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//batch_size}: {str(e)}\")\n",
    "                perplexities.extend([1000.0] * len(batch))\n",
    "        \n",
    "        return perplexities\n",
    "\n",
    "perplexity_calculator = PerplexityCalculator()\n",
    "\n",
    "# ------ Tag Presence Reward ------\n",
    "def tag_presence_reward(completions: List[dict]) -> List[float]:\n",
    "    \"\"\"Reward for presence of <reasoning> and <answer> tags\"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        content = completion[0]['content']\n",
    "        has_reasoning = bool(re.search(r'<reasoning>.*?</reasoning>', content, re.DOTALL))\n",
    "        has_answer = bool(re.search(r'<answer>.*?</answer>', content, re.DOTALL))\n",
    "        reward = 0.5 * has_reasoning + 0.5 * has_answer\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "# ------ Combined Reward Function ------\n",
    "def combined_reward_func(\n",
    "    prompts, completions, answer, **kwargs\n",
    ") -> List[float]:\n",
    "    # Extract generated responses\n",
    "    responses = []\n",
    "    valid_indices = []\n",
    "    for idx, completion in enumerate(completions):\n",
    "        try:\n",
    "            generated_content = completion[0]['content'].strip()\n",
    "\n",
    "            # Extract only the <answer> content using regex\n",
    "            answer_match = re.search(r'<answer>(.*?)</answer>', generated_content, re.DOTALL)\n",
    "            if answer_match:\n",
    "                generated_content = answer_match.group(1).strip()\n",
    "            else:\n",
    "                responses.append(\"\")  # Append empty string to maintain index consistency\n",
    "                valid_indices.append(idx)\n",
    "                continue\n",
    "            \n",
    "            # Skip if empty or just repeating the prompt\n",
    "            user_prompt = prompts[idx][-1]['content']\n",
    "            if not generated_content or generated_content == user_prompt:\n",
    "                # print(f\"generated_content: continue\")\n",
    "                responses.append(\"\")  # Append empty string to maintain index consistency\n",
    "                valid_indices.append(idx)\n",
    "                continue\n",
    "                \n",
    "            responses.append(generated_content)\n",
    "            # print(f\"generated_content3: {generated_content}\")\n",
    "            valid_indices.append(idx)\n",
    "        except (KeyError, IndexError):\n",
    "            responses.append(\"\")  # Append empty string to maintain index consistency\n",
    "            valid_indices.append(idx)\n",
    "            continue\n",
    "    \n",
    "    if not responses:\n",
    "        return [-1.0] * len(completions)\n",
    "    \n",
    "    # Calculate rewards\n",
    "    try:\n",
    "        # Use raw answers without tag processing\n",
    "        processed_answers = answer\n",
    "        # print(f\"processed_answers: {processed_answers[0]}\")\n",
    "        \n",
    "        similarities = semantic_correctness(responses, [processed_answers[i] for i in valid_indices])\n",
    "        perplexities = perplexity_calculator.calculate([processed_answers[i] for i in valid_indices])\n",
    "        tag_rewards = tag_presence_reward([completions[i] for i in valid_indices])\n",
    "    except Exception as e:\n",
    "        print(f\"Reward calculation error: {str(e)}\")\n",
    "        return [-1.0] * len(completions)\n",
    "    \n",
    "    # Convert to tensors with stability\n",
    "    sim_scores = torch.nan_to_num(torch.tensor(similarities), nan=0.0)\n",
    "    perplex_scores = torch.nan_to_num(torch.tensor(perplexities), nan=1000.0)\n",
    "    tag_scores = torch.tensor(tag_rewards)\n",
    "        \n",
    "    # Perplexity reward calculation\n",
    "    perplex_rewards = 1 / (perplex_scores / (perplex_scores.mean() + 1e-9))\n",
    "    \n",
    "    # Normalize with stability\n",
    "    score_range = perplex_rewards.max() - perplex_rewards.min()\n",
    "    if score_range < 1e-6:\n",
    "        perplex_rewards_normalized = torch.ones_like(perplex_rewards) * 0.5\n",
    "    else:\n",
    "        perplex_rewards_normalized = (perplex_rewards - perplex_rewards.min()) / score_range\n",
    "    \n",
    "    # Combine scores with validation\n",
    "    combined = [\n",
    "        0.5 * sim.item() + 0.4 * pr.item() + 0.1 * tag.item()\n",
    "        for sim, pr, tag in zip(sim_scores, perplex_rewards_normalized, tag_scores)\n",
    "        if not torch.isnan(sim) and not torch.isnan(pr) and not torch.isnan(tag)\n",
    "    ]\n",
    "    \n",
    "    # Map back to original indices\n",
    "    final_rewards = [-1.0] * len(completions)\n",
    "    for idx, reward in zip(valid_indices, combined):\n",
    "        final_rewards[idx] = max(min(reward, 1.0), -1.0)  # Clamp between -1 and 1\n",
    "    \n",
    "    assert len(final_rewards) == len(completions), \"Reward mapping error\"\n",
    "    return final_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3685be6",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "### Now set up GRPO Trainer and all configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1498bb02-394d-4318-b023-7d3b36b55876",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 2 # 2 for quicker iterations, 4 otherwise\n",
    "\n",
    "total_steps = 200  # Changed from max steps of 1000\n",
    "num_checkpoints = 4\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm=True,  # use vLLM for fast inference!\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_generations=5,  # Decrease if out of memory\n",
    "    max_prompt_length=128,  # Updated by lowering to 128 from 512 to balance longer input prompts with training time requirements\n",
    "    max_completion_length=128,\n",
    "    max_steps=total_steps,\n",
    "    save_steps=int(total_steps // num_checkpoints),\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"none\",  # Can use Weights & Biases\n",
    "    output_dir=\"grpo_outputs\",\n",
    "    save_strategy=\"steps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a3ec0d",
   "metadata": {},
   "source": [
    "## Let's run the trainer. \n",
    "### If you scroll, you'll see a table of rewards. The goal is to see the reward column increase. You might have to wait 150 to 200 steps for any action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0575909-1ced-48e9-89af-9978454d207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        combined_reward_func\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a46eeff-1fc2-47f2-ba15-8e96a0496a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 25,117 | Num Epochs = 1 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 2 x 1) = 2\n",
      " \"-____-\"     Trainable parameters = 167,772,160/8,000,000,000 (2.10% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 3:16:13, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completion_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / combined_reward_func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.205303</td>\n",
       "      <td>0.211749</td>\n",
       "      <td>121.900002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.205303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>124.600002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.206189</td>\n",
       "      <td>0.196091</td>\n",
       "      <td>113.900002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.206189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167601</td>\n",
       "      <td>0.410170</td>\n",
       "      <td>112.900002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.167601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.209033</td>\n",
       "      <td>0.203408</td>\n",
       "      <td>113.000004</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.209033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.121850</td>\n",
       "      <td>0.243961</td>\n",
       "      <td>126.100002</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-0.121850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.205588</td>\n",
       "      <td>0.197433</td>\n",
       "      <td>123.600002</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.205588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>119.799999</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.015486</td>\n",
       "      <td>0.237122</td>\n",
       "      <td>108.099998</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.015486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.211745</td>\n",
       "      <td>0.197345</td>\n",
       "      <td>121.400002</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>-0.211745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.136868</td>\n",
       "      <td>0.323690</td>\n",
       "      <td>100.100002</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>-0.136868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>127.400002</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.285000</td>\n",
       "      <td>0.013693</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.280000</td>\n",
       "      <td>0.024873</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.106101</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>109.299999</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>-0.106101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.295000</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>-0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.141911</td>\n",
       "      <td>0.220992</td>\n",
       "      <td>103.500000</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>-0.141911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.204273</td>\n",
       "      <td>0.200369</td>\n",
       "      <td>122.900002</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>-0.204273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.295000</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>124.600002</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>-0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.209669</td>\n",
       "      <td>0.188342</td>\n",
       "      <td>122.799999</td>\n",
       "      <td>0.004712</td>\n",
       "      <td>-0.209669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.075778</td>\n",
       "      <td>0.462246</td>\n",
       "      <td>94.200001</td>\n",
       "      <td>0.008508</td>\n",
       "      <td>0.075778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-0.197021</td>\n",
       "      <td>0.216577</td>\n",
       "      <td>126.200001</td>\n",
       "      <td>0.011972</td>\n",
       "      <td>-0.197021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.205277</td>\n",
       "      <td>0.211807</td>\n",
       "      <td>126.600002</td>\n",
       "      <td>0.005367</td>\n",
       "      <td>-0.205277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>-0.295000</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.007697</td>\n",
       "      <td>-0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.066624</td>\n",
       "      <td>0.450074</td>\n",
       "      <td>120.900002</td>\n",
       "      <td>0.005483</td>\n",
       "      <td>0.066624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-0.290000</td>\n",
       "      <td>0.022361</td>\n",
       "      <td>121.500000</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>-0.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>-0.171155</td>\n",
       "      <td>0.226607</td>\n",
       "      <td>121.799999</td>\n",
       "      <td>0.022133</td>\n",
       "      <td>-0.171155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.201696</td>\n",
       "      <td>0.206123</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>-0.201696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>-0.188362</td>\n",
       "      <td>0.208002</td>\n",
       "      <td>105.900002</td>\n",
       "      <td>0.048348</td>\n",
       "      <td>-0.188362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-0.184100</td>\n",
       "      <td>0.198087</td>\n",
       "      <td>127.799999</td>\n",
       "      <td>0.014185</td>\n",
       "      <td>-0.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>-0.001314</td>\n",
       "      <td>0.428878</td>\n",
       "      <td>123.600002</td>\n",
       "      <td>0.025165</td>\n",
       "      <td>-0.001314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-0.050293</td>\n",
       "      <td>0.228079</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.015181</td>\n",
       "      <td>-0.050293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.513711</td>\n",
       "      <td>0.213582</td>\n",
       "      <td>111.500000</td>\n",
       "      <td>0.028430</td>\n",
       "      <td>0.513711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.185208</td>\n",
       "      <td>0.022404</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.039294</td>\n",
       "      <td>0.185208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>-0.108384</td>\n",
       "      <td>0.259351</td>\n",
       "      <td>104.500000</td>\n",
       "      <td>0.042824</td>\n",
       "      <td>-0.108384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>-0.012173</td>\n",
       "      <td>0.418939</td>\n",
       "      <td>122.799999</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>-0.012173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.251999</td>\n",
       "      <td>0.443363</td>\n",
       "      <td>117.600002</td>\n",
       "      <td>0.034506</td>\n",
       "      <td>0.251999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-0.109676</td>\n",
       "      <td>0.249410</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.014171</td>\n",
       "      <td>-0.109676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>-0.090194</td>\n",
       "      <td>0.264757</td>\n",
       "      <td>122.200001</td>\n",
       "      <td>0.016695</td>\n",
       "      <td>-0.090194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.062094</td>\n",
       "      <td>0.189220</td>\n",
       "      <td>124.600002</td>\n",
       "      <td>0.028775</td>\n",
       "      <td>0.062094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.327528</td>\n",
       "      <td>0.241430</td>\n",
       "      <td>90.600000</td>\n",
       "      <td>0.064392</td>\n",
       "      <td>0.327528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.322665</td>\n",
       "      <td>0.417593</td>\n",
       "      <td>116.600002</td>\n",
       "      <td>0.056148</td>\n",
       "      <td>0.322665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.162302</td>\n",
       "      <td>0.033724</td>\n",
       "      <td>117.800003</td>\n",
       "      <td>0.042442</td>\n",
       "      <td>0.162302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.081948</td>\n",
       "      <td>0.448361</td>\n",
       "      <td>126.400002</td>\n",
       "      <td>0.059159</td>\n",
       "      <td>0.081948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.058651</td>\n",
       "      <td>0.379209</td>\n",
       "      <td>117.900002</td>\n",
       "      <td>0.068238</td>\n",
       "      <td>0.058651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.076558</td>\n",
       "      <td>0.427696</td>\n",
       "      <td>126.900002</td>\n",
       "      <td>0.060004</td>\n",
       "      <td>0.076558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>-0.005911</td>\n",
       "      <td>0.222826</td>\n",
       "      <td>127.299999</td>\n",
       "      <td>0.032535</td>\n",
       "      <td>-0.005911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>-0.071228</td>\n",
       "      <td>0.256408</td>\n",
       "      <td>127.900002</td>\n",
       "      <td>0.043342</td>\n",
       "      <td>-0.071228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>-0.190910</td>\n",
       "      <td>0.221572</td>\n",
       "      <td>95.900000</td>\n",
       "      <td>0.051026</td>\n",
       "      <td>-0.190910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.163890</td>\n",
       "      <td>0.022517</td>\n",
       "      <td>103.799999</td>\n",
       "      <td>0.090559</td>\n",
       "      <td>0.163890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.192637</td>\n",
       "      <td>0.481690</td>\n",
       "      <td>98.800003</td>\n",
       "      <td>0.074080</td>\n",
       "      <td>0.192637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.618831</td>\n",
       "      <td>0.011419</td>\n",
       "      <td>112.299999</td>\n",
       "      <td>0.086059</td>\n",
       "      <td>0.618831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.417784</td>\n",
       "      <td>0.200073</td>\n",
       "      <td>115.400002</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.417784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.554259</td>\n",
       "      <td>0.192559</td>\n",
       "      <td>112.200001</td>\n",
       "      <td>0.054056</td>\n",
       "      <td>0.554259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.611373</td>\n",
       "      <td>0.039653</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.062864</td>\n",
       "      <td>0.611373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.226177</td>\n",
       "      <td>121.600002</td>\n",
       "      <td>0.076193</td>\n",
       "      <td>0.552326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.547338</td>\n",
       "      <td>0.201158</td>\n",
       "      <td>93.200001</td>\n",
       "      <td>0.070438</td>\n",
       "      <td>0.547338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.633206</td>\n",
       "      <td>0.018088</td>\n",
       "      <td>106.600002</td>\n",
       "      <td>0.086764</td>\n",
       "      <td>0.633206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.197920</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>105.299999</td>\n",
       "      <td>0.069517</td>\n",
       "      <td>0.197920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.503820</td>\n",
       "      <td>0.207790</td>\n",
       "      <td>126.600002</td>\n",
       "      <td>0.043496</td>\n",
       "      <td>0.503820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.172855</td>\n",
       "      <td>0.013693</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.095316</td>\n",
       "      <td>0.172855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.276158</td>\n",
       "      <td>0.480385</td>\n",
       "      <td>120.300003</td>\n",
       "      <td>0.073816</td>\n",
       "      <td>0.276158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>0.255373</td>\n",
       "      <td>115.700001</td>\n",
       "      <td>0.126837</td>\n",
       "      <td>0.403034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>-0.086368</td>\n",
       "      <td>0.224111</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.090871</td>\n",
       "      <td>-0.086368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.460160</td>\n",
       "      <td>0.230846</td>\n",
       "      <td>105.299999</td>\n",
       "      <td>0.051002</td>\n",
       "      <td>0.460160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.179338</td>\n",
       "      <td>0.391896</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.047859</td>\n",
       "      <td>0.179338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.274326</td>\n",
       "      <td>0.181299</td>\n",
       "      <td>121.100002</td>\n",
       "      <td>0.077156</td>\n",
       "      <td>0.274326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.641862</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>121.100002</td>\n",
       "      <td>0.069640</td>\n",
       "      <td>0.641862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.123232</td>\n",
       "      <td>0.208910</td>\n",
       "      <td>113.200001</td>\n",
       "      <td>0.095717</td>\n",
       "      <td>0.123232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.444509</td>\n",
       "      <td>0.263764</td>\n",
       "      <td>113.000004</td>\n",
       "      <td>0.092707</td>\n",
       "      <td>0.444509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.075435</td>\n",
       "      <td>0.197014</td>\n",
       "      <td>120.600002</td>\n",
       "      <td>0.092572</td>\n",
       "      <td>0.075435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.540928</td>\n",
       "      <td>0.221893</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.092953</td>\n",
       "      <td>0.540928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.536148</td>\n",
       "      <td>0.223040</td>\n",
       "      <td>115.300003</td>\n",
       "      <td>0.091055</td>\n",
       "      <td>0.536148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.401967</td>\n",
       "      <td>0.244695</td>\n",
       "      <td>111.500000</td>\n",
       "      <td>0.074403</td>\n",
       "      <td>0.401967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.421207</td>\n",
       "      <td>0.267601</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.231455</td>\n",
       "      <td>0.421207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.090514</td>\n",
       "      <td>0.210062</td>\n",
       "      <td>122.799999</td>\n",
       "      <td>0.125932</td>\n",
       "      <td>0.090514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.271063</td>\n",
       "      <td>0.193532</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.103539</td>\n",
       "      <td>0.271063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.431072</td>\n",
       "      <td>0.388837</td>\n",
       "      <td>115.700001</td>\n",
       "      <td>0.323561</td>\n",
       "      <td>0.431072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.407990</td>\n",
       "      <td>0.239106</td>\n",
       "      <td>124.299999</td>\n",
       "      <td>0.053353</td>\n",
       "      <td>0.407990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.417306</td>\n",
       "      <td>0.264315</td>\n",
       "      <td>107.100002</td>\n",
       "      <td>0.097610</td>\n",
       "      <td>0.417306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.443215</td>\n",
       "      <td>0.262193</td>\n",
       "      <td>118.299999</td>\n",
       "      <td>0.086669</td>\n",
       "      <td>0.443215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.147097</td>\n",
       "      <td>0.019348</td>\n",
       "      <td>127.200001</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.147097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.415093</td>\n",
       "      <td>0.171069</td>\n",
       "      <td>88.400000</td>\n",
       "      <td>0.059721</td>\n",
       "      <td>0.415093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.214923</td>\n",
       "      <td>0.019975</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.085497</td>\n",
       "      <td>0.214923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.563560</td>\n",
       "      <td>0.219568</td>\n",
       "      <td>117.900002</td>\n",
       "      <td>0.108553</td>\n",
       "      <td>0.563560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.091974</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>123.400002</td>\n",
       "      <td>0.098758</td>\n",
       "      <td>0.091974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>-0.009434</td>\n",
       "      <td>0.407735</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.078774</td>\n",
       "      <td>-0.009434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.240416</td>\n",
       "      <td>0.434561</td>\n",
       "      <td>127.700001</td>\n",
       "      <td>0.122804</td>\n",
       "      <td>0.240416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.506674</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>117.500000</td>\n",
       "      <td>0.104437</td>\n",
       "      <td>0.506674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.004367</td>\n",
       "      <td>0.232312</td>\n",
       "      <td>119.700001</td>\n",
       "      <td>0.114688</td>\n",
       "      <td>0.004367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.433975</td>\n",
       "      <td>0.382821</td>\n",
       "      <td>111.500004</td>\n",
       "      <td>0.127026</td>\n",
       "      <td>0.433975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.105792</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>127.799999</td>\n",
       "      <td>0.054037</td>\n",
       "      <td>0.105792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.531535</td>\n",
       "      <td>0.217701</td>\n",
       "      <td>105.500000</td>\n",
       "      <td>0.101380</td>\n",
       "      <td>0.531535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.571887</td>\n",
       "      <td>0.021428</td>\n",
       "      <td>124.900002</td>\n",
       "      <td>0.093070</td>\n",
       "      <td>0.571887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.588192</td>\n",
       "      <td>0.080443</td>\n",
       "      <td>104.299999</td>\n",
       "      <td>0.101825</td>\n",
       "      <td>0.588192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.272249</td>\n",
       "      <td>0.488715</td>\n",
       "      <td>120.300003</td>\n",
       "      <td>0.084641</td>\n",
       "      <td>0.272249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.509217</td>\n",
       "      <td>0.228152</td>\n",
       "      <td>102.400002</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>0.509217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.613805</td>\n",
       "      <td>0.006196</td>\n",
       "      <td>104.400002</td>\n",
       "      <td>0.068314</td>\n",
       "      <td>0.613805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.099894</td>\n",
       "      <td>0.350463</td>\n",
       "      <td>122.900002</td>\n",
       "      <td>0.125505</td>\n",
       "      <td>0.099894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.604488</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.063143</td>\n",
       "      <td>0.604488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.142303</td>\n",
       "      <td>0.426449</td>\n",
       "      <td>125.200001</td>\n",
       "      <td>0.060515</td>\n",
       "      <td>0.142303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.503934</td>\n",
       "      <td>0.252202</td>\n",
       "      <td>108.700005</td>\n",
       "      <td>0.105074</td>\n",
       "      <td>0.503934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.343693</td>\n",
       "      <td>0.262427</td>\n",
       "      <td>125.900002</td>\n",
       "      <td>0.093082</td>\n",
       "      <td>0.343693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.600357</td>\n",
       "      <td>0.024509</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>0.060687</td>\n",
       "      <td>0.600357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.062115</td>\n",
       "      <td>0.174509</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.062115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.575705</td>\n",
       "      <td>0.031234</td>\n",
       "      <td>122.600002</td>\n",
       "      <td>0.087254</td>\n",
       "      <td>0.575705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.186846</td>\n",
       "      <td>0.025896</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>0.184151</td>\n",
       "      <td>0.186846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.459627</td>\n",
       "      <td>0.214746</td>\n",
       "      <td>108.600002</td>\n",
       "      <td>0.082768</td>\n",
       "      <td>0.459627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.144799</td>\n",
       "      <td>0.052440</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.098688</td>\n",
       "      <td>0.144799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.114443</td>\n",
       "      <td>0.403569</td>\n",
       "      <td>121.400002</td>\n",
       "      <td>0.130306</td>\n",
       "      <td>0.114443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.424185</td>\n",
       "      <td>0.364208</td>\n",
       "      <td>122.700001</td>\n",
       "      <td>0.086628</td>\n",
       "      <td>0.424185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.553803</td>\n",
       "      <td>0.202883</td>\n",
       "      <td>117.799999</td>\n",
       "      <td>0.126507</td>\n",
       "      <td>0.553803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.637066</td>\n",
       "      <td>0.031383</td>\n",
       "      <td>103.400002</td>\n",
       "      <td>0.137769</td>\n",
       "      <td>0.637066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.326315</td>\n",
       "      <td>0.280009</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.109018</td>\n",
       "      <td>0.326315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.576690</td>\n",
       "      <td>0.094815</td>\n",
       "      <td>98.600002</td>\n",
       "      <td>0.103299</td>\n",
       "      <td>0.576690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.511386</td>\n",
       "      <td>0.201421</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.103917</td>\n",
       "      <td>0.511386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.276310</td>\n",
       "      <td>0.193784</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.093899</td>\n",
       "      <td>0.276310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.379989</td>\n",
       "      <td>0.247326</td>\n",
       "      <td>98.799999</td>\n",
       "      <td>0.137274</td>\n",
       "      <td>0.379989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.276684</td>\n",
       "      <td>0.430745</td>\n",
       "      <td>125.100002</td>\n",
       "      <td>0.093803</td>\n",
       "      <td>0.276684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.173516</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.098348</td>\n",
       "      <td>0.173516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.331866</td>\n",
       "      <td>0.210415</td>\n",
       "      <td>122.100002</td>\n",
       "      <td>0.065827</td>\n",
       "      <td>0.331866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.421575</td>\n",
       "      <td>0.221024</td>\n",
       "      <td>102.799999</td>\n",
       "      <td>0.211874</td>\n",
       "      <td>0.421575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.399388</td>\n",
       "      <td>0.256855</td>\n",
       "      <td>85.100000</td>\n",
       "      <td>0.063903</td>\n",
       "      <td>0.399388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.585286</td>\n",
       "      <td>0.039176</td>\n",
       "      <td>120.299999</td>\n",
       "      <td>0.072511</td>\n",
       "      <td>0.585286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.198668</td>\n",
       "      <td>0.491437</td>\n",
       "      <td>125.299999</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.198668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.576243</td>\n",
       "      <td>0.008411</td>\n",
       "      <td>94.200003</td>\n",
       "      <td>0.069426</td>\n",
       "      <td>0.576243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.491837</td>\n",
       "      <td>0.229037</td>\n",
       "      <td>119.300003</td>\n",
       "      <td>0.087691</td>\n",
       "      <td>0.491837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.578572</td>\n",
       "      <td>0.114545</td>\n",
       "      <td>121.900002</td>\n",
       "      <td>0.091916</td>\n",
       "      <td>0.578572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.064993</td>\n",
       "      <td>0.584337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.330835</td>\n",
       "      <td>0.201034</td>\n",
       "      <td>122.900002</td>\n",
       "      <td>0.091784</td>\n",
       "      <td>0.330835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.610546</td>\n",
       "      <td>0.026130</td>\n",
       "      <td>120.900002</td>\n",
       "      <td>0.101829</td>\n",
       "      <td>0.610546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.639707</td>\n",
       "      <td>0.022218</td>\n",
       "      <td>97.100002</td>\n",
       "      <td>0.146386</td>\n",
       "      <td>0.639707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.382645</td>\n",
       "      <td>0.442745</td>\n",
       "      <td>119.200001</td>\n",
       "      <td>0.120491</td>\n",
       "      <td>0.382645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>-0.019979</td>\n",
       "      <td>0.221668</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.068377</td>\n",
       "      <td>-0.019979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.511850</td>\n",
       "      <td>0.197352</td>\n",
       "      <td>107.800003</td>\n",
       "      <td>0.059095</td>\n",
       "      <td>0.511850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.271813</td>\n",
       "      <td>0.173116</td>\n",
       "      <td>127.600002</td>\n",
       "      <td>0.123718</td>\n",
       "      <td>0.271813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.264211</td>\n",
       "      <td>0.296136</td>\n",
       "      <td>87.900000</td>\n",
       "      <td>0.210475</td>\n",
       "      <td>0.264211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.621355</td>\n",
       "      <td>0.030896</td>\n",
       "      <td>110.600002</td>\n",
       "      <td>0.094119</td>\n",
       "      <td>0.621355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.596523</td>\n",
       "      <td>0.030812</td>\n",
       "      <td>117.900002</td>\n",
       "      <td>0.068421</td>\n",
       "      <td>0.596523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.466596</td>\n",
       "      <td>0.256778</td>\n",
       "      <td>124.900002</td>\n",
       "      <td>0.061427</td>\n",
       "      <td>0.466596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.333606</td>\n",
       "      <td>0.240936</td>\n",
       "      <td>118.600002</td>\n",
       "      <td>0.100415</td>\n",
       "      <td>0.333606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.285762</td>\n",
       "      <td>0.199062</td>\n",
       "      <td>111.600002</td>\n",
       "      <td>0.098408</td>\n",
       "      <td>0.285762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.598785</td>\n",
       "      <td>0.043527</td>\n",
       "      <td>111.500004</td>\n",
       "      <td>0.117505</td>\n",
       "      <td>0.598785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.535536</td>\n",
       "      <td>0.186157</td>\n",
       "      <td>115.500000</td>\n",
       "      <td>0.068839</td>\n",
       "      <td>0.535536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.562599</td>\n",
       "      <td>0.216656</td>\n",
       "      <td>118.299999</td>\n",
       "      <td>0.077220</td>\n",
       "      <td>0.562599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.430460</td>\n",
       "      <td>0.226333</td>\n",
       "      <td>105.400002</td>\n",
       "      <td>0.076377</td>\n",
       "      <td>0.430460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.623548</td>\n",
       "      <td>0.033617</td>\n",
       "      <td>126.600002</td>\n",
       "      <td>0.063770</td>\n",
       "      <td>0.623548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.506819</td>\n",
       "      <td>0.228832</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.078811</td>\n",
       "      <td>0.506819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.670332</td>\n",
       "      <td>0.044315</td>\n",
       "      <td>82.000004</td>\n",
       "      <td>0.080469</td>\n",
       "      <td>0.670332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.183166</td>\n",
       "      <td>0.399460</td>\n",
       "      <td>125.400002</td>\n",
       "      <td>0.097340</td>\n",
       "      <td>0.183166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.199693</td>\n",
       "      <td>0.403554</td>\n",
       "      <td>119.100002</td>\n",
       "      <td>0.127147</td>\n",
       "      <td>0.199693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.436403</td>\n",
       "      <td>0.247418</td>\n",
       "      <td>96.700001</td>\n",
       "      <td>0.101104</td>\n",
       "      <td>0.436403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.667792</td>\n",
       "      <td>0.030365</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.097859</td>\n",
       "      <td>0.667792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.547948</td>\n",
       "      <td>0.198726</td>\n",
       "      <td>104.900002</td>\n",
       "      <td>0.112564</td>\n",
       "      <td>0.547948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.593115</td>\n",
       "      <td>0.010408</td>\n",
       "      <td>127.799999</td>\n",
       "      <td>0.083083</td>\n",
       "      <td>0.593115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.277817</td>\n",
       "      <td>0.207940</td>\n",
       "      <td>108.200001</td>\n",
       "      <td>0.127174</td>\n",
       "      <td>0.277817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.578154</td>\n",
       "      <td>0.019353</td>\n",
       "      <td>98.900002</td>\n",
       "      <td>0.094757</td>\n",
       "      <td>0.578154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.432801</td>\n",
       "      <td>0.255802</td>\n",
       "      <td>97.100002</td>\n",
       "      <td>0.171878</td>\n",
       "      <td>0.432801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.537149</td>\n",
       "      <td>0.213836</td>\n",
       "      <td>115.400002</td>\n",
       "      <td>0.088825</td>\n",
       "      <td>0.537149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>0.346972</td>\n",
       "      <td>0.267568</td>\n",
       "      <td>117.500004</td>\n",
       "      <td>2.761441</td>\n",
       "      <td>0.346972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.473320</td>\n",
       "      <td>0.282778</td>\n",
       "      <td>109.299999</td>\n",
       "      <td>0.070591</td>\n",
       "      <td>0.473320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.521368</td>\n",
       "      <td>0.208816</td>\n",
       "      <td>124.500000</td>\n",
       "      <td>0.162093</td>\n",
       "      <td>0.521368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.062470</td>\n",
       "      <td>0.431203</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>0.097085</td>\n",
       "      <td>0.062470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.375090</td>\n",
       "      <td>0.256325</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>0.107908</td>\n",
       "      <td>0.375090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.522246</td>\n",
       "      <td>0.213303</td>\n",
       "      <td>127.500000</td>\n",
       "      <td>0.077240</td>\n",
       "      <td>0.522246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.361369</td>\n",
       "      <td>0.421307</td>\n",
       "      <td>118.300003</td>\n",
       "      <td>0.105961</td>\n",
       "      <td>0.361369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.415992</td>\n",
       "      <td>0.269246</td>\n",
       "      <td>87.400002</td>\n",
       "      <td>0.082619</td>\n",
       "      <td>0.415992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.294682</td>\n",
       "      <td>0.283677</td>\n",
       "      <td>125.100002</td>\n",
       "      <td>0.158079</td>\n",
       "      <td>0.294682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.352884</td>\n",
       "      <td>0.255991</td>\n",
       "      <td>121.500004</td>\n",
       "      <td>0.072731</td>\n",
       "      <td>0.352884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.267907</td>\n",
       "      <td>0.486241</td>\n",
       "      <td>110.500004</td>\n",
       "      <td>0.219594</td>\n",
       "      <td>0.267907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.588516</td>\n",
       "      <td>0.041757</td>\n",
       "      <td>119.100002</td>\n",
       "      <td>0.128591</td>\n",
       "      <td>0.588516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.506273</td>\n",
       "      <td>0.216834</td>\n",
       "      <td>110.800003</td>\n",
       "      <td>0.284742</td>\n",
       "      <td>0.506273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.448544</td>\n",
       "      <td>0.240074</td>\n",
       "      <td>113.600002</td>\n",
       "      <td>0.075371</td>\n",
       "      <td>0.448544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.162906</td>\n",
       "      <td>0.238489</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.064389</td>\n",
       "      <td>0.162906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.628914</td>\n",
       "      <td>0.018052</td>\n",
       "      <td>101.100002</td>\n",
       "      <td>0.117567</td>\n",
       "      <td>0.628914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.429238</td>\n",
       "      <td>0.380087</td>\n",
       "      <td>120.299999</td>\n",
       "      <td>0.140856</td>\n",
       "      <td>0.429238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.599026</td>\n",
       "      <td>0.026942</td>\n",
       "      <td>93.100002</td>\n",
       "      <td>0.160385</td>\n",
       "      <td>0.599026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>-0.030044</td>\n",
       "      <td>0.230242</td>\n",
       "      <td>126.100002</td>\n",
       "      <td>0.113395</td>\n",
       "      <td>-0.030044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.374850</td>\n",
       "      <td>0.243901</td>\n",
       "      <td>116.900002</td>\n",
       "      <td>0.088680</td>\n",
       "      <td>0.374850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.570244</td>\n",
       "      <td>0.039448</td>\n",
       "      <td>110.700001</td>\n",
       "      <td>0.132932</td>\n",
       "      <td>0.570244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.554849</td>\n",
       "      <td>0.029338</td>\n",
       "      <td>92.200001</td>\n",
       "      <td>0.082356</td>\n",
       "      <td>0.554849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.504315</td>\n",
       "      <td>0.222571</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.099873</td>\n",
       "      <td>0.504315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.076085</td>\n",
       "      <td>0.206845</td>\n",
       "      <td>125.600002</td>\n",
       "      <td>0.104854</td>\n",
       "      <td>0.076085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.504021</td>\n",
       "      <td>0.185124</td>\n",
       "      <td>110.799999</td>\n",
       "      <td>0.088475</td>\n",
       "      <td>0.504021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.648369</td>\n",
       "      <td>0.021819</td>\n",
       "      <td>114.400002</td>\n",
       "      <td>0.184216</td>\n",
       "      <td>0.648369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.520255</td>\n",
       "      <td>0.191084</td>\n",
       "      <td>112.400002</td>\n",
       "      <td>0.095207</td>\n",
       "      <td>0.520255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.524657</td>\n",
       "      <td>0.222910</td>\n",
       "      <td>121.700001</td>\n",
       "      <td>0.056359</td>\n",
       "      <td>0.524657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.618838</td>\n",
       "      <td>0.035583</td>\n",
       "      <td>119.900002</td>\n",
       "      <td>0.130519</td>\n",
       "      <td>0.618838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>-0.001053</td>\n",
       "      <td>0.416618</td>\n",
       "      <td>122.799999</td>\n",
       "      <td>0.069136</td>\n",
       "      <td>-0.001053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.192707</td>\n",
       "      <td>0.508925</td>\n",
       "      <td>125.700005</td>\n",
       "      <td>0.180419</td>\n",
       "      <td>0.192707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.571347</td>\n",
       "      <td>0.033743</td>\n",
       "      <td>116.800003</td>\n",
       "      <td>0.074221</td>\n",
       "      <td>0.571347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.446418</td>\n",
       "      <td>0.403783</td>\n",
       "      <td>114.200001</td>\n",
       "      <td>0.131499</td>\n",
       "      <td>0.446418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.455608</td>\n",
       "      <td>0.246828</td>\n",
       "      <td>125.200001</td>\n",
       "      <td>0.117190</td>\n",
       "      <td>0.455608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>-0.177071</td>\n",
       "      <td>0.163073</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.054522</td>\n",
       "      <td>-0.177071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.0036339496986677756, metrics={'train_runtime': 11860.4504, 'train_samples_per_second': 0.034, 'train_steps_per_second': 0.017, 'total_flos': 0.0, 'train_loss': 0.0036339496986677756})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f37b6",
   "metadata": {},
   "source": [
    "## Inference\n",
    "### Now let's try the model we just trained: First, let's first try the model without any GRPO trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3aa44e9c-83dd-4324-ae6b-60026fd656a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆ| 1/1 [00:24<00:00, 24.88s/it, est. speed input: 1.85 t\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"Is Aspirin good for cardio vascular function?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1561145b-d518-4c2c-814f-86ea68b76ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aspirin's effect on cardiovascular function is complex and multifaceted. In small doses, aspirin is often recommended for people at risk of heart disease or those who have already had a heart attack or stroke. It acts as an antiplatelet agent, which helps prevent blood clots from forming and reducing the risk of heart attacks and strokes.\\n\\nHowever, it is essential to note that aspirin is not suitable for everyone, particularly those with a history of stomach ulcers or bleeding disorders. It's also not recommended for healthy individuals, as the risks may outweigh the benefits.\\n\\nAspirin's benefits for cardiovascular function include:\\n\\n1. **Antiplatelet activity**: Aspirin inhibits the production of thromboxane A2, a compound that promotes blood clotting, thus reducing the risk of blood clots forming.\\n2. **Inflammation reduction**: Aspirin has anti-inflammatory properties, which can help reduce inflammation in blood vessels, a risk factor for cardiovascular disease.\\n3. **Blood vessel dilation**: Aspirin can cause blood vessels to relax, improving blood flow and lowering blood pressure.\\n\\nHowever, there are also potential drawbacks to consider:\\n\\n1. **Gastrointestinal side effects**: Long-term aspirin use can lead to stomach ulcers, bleeding, and other gastrointestinal issues.\\n2. **Bleeding risk**: Aspirin can increase the risk of bleeding, particularly in older adults or those with bleeding disorders.\\n3. **Overuse**: Taking too much aspirin can lead to excessive bleeding, which can be severe and even life-threatening.\\n\\nTo determine whether aspirin is suitable for you, consult with your doctor or healthcare provider, who will assess your individual risk factors and medical history. They will help you decide whether the benefits of aspirin outweigh the risks.\\n\\n**Who should consider taking aspirin for cardiovascular health?**\\n\\n* People with a history of heart disease, stroke, or transient ischemic attack (TIA)\\n* Those with a family history of cardiovascular disease\\n* Individuals with diabetes, high blood pressure, or high cholesterol\\n* People with peripheral artery disease (PAD) or carotid artery disease\\n\\n**Who should not take aspirin?**\\n\\n* Those with a history of stomach ulcers or bleeding disorders\\n* Older adults with a history of falls or other bleeding risks\\n* Pregnant or breastfeeding women\\n* Individuals with kidney or liver disease\\n* People taking anticoagulant or antiplatelet medications without a valid medical reason\\n\\nIt is crucial to consult with your healthcare provider to weigh the benefits and risks of aspirin for your specific situation.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff821fe",
   "metadata": {},
   "source": [
    "## And now with the LoRA we just trained with GRPO - we save the LoRA first:\n",
    "### Now we load the LoRA and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db665710-8829-463f-87ec-94b8ff86d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c350c8b1-fd57-4da0-ab61-1debba92083a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆ| 1/1 [00:26<00:00, 26.60s/it, est. speed input: 2.56 t\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : \"Is Aspirin good for cardio vascular function?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46804d00-51c1-4e26-bea2-b990a01a97af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<reasoning>\\nAspirin has been a topic of interest in the context of cardiovascular health for many years.\\n</reasoning>\\n<answer>\\nAspirin may have both positive and negative effects on cardiovascular function, depending on the individual and their specific health status.\\n</answer>\\n\\nPositive effects:\\n\\n1. **Anti-clotting properties**: Aspirin's antiplatelet effect can help prevent blood clots from forming, reducing the risk of heart attacks and strokes in people with a history of cardiovascular disease.\\n2. **Reducing inflammation**: Aspirin has anti-inflammatory properties, which may help lower the risk of cardiovascular disease by reducing inflammation in the blood vessels.\\n3. **Lowering blood pressure**: Regular, low-dose aspirin use may help lower blood pressure in people with hypertension.\\n\\nNegative effects:\\n\\n1. **Bleeding risks**: Aspirin can increase the risk of bleeding, especially in people with a history of gastrointestinal problems or taking other medications that affect blood clotting.\\n2. **Gastrointestinal side effects**: Long-term aspirin use can cause stomach ulcers, bleeding in the stomach or intestines, and other gastrointestinal problems.\\n3. **Interactions with other medications**: Aspirin can interact with other medications, such as blood thinners, and increase the risk of bleeding or other complications.\\n\\n**Who should take aspirin for cardiovascular health?**\\n\\nAspirin may be recommended for:\\n\\n1. **People with a history of cardiovascular disease**: Those who have had a heart attack, stroke, or other cardiovascular event may benefit from aspirin's anti-clotting properties.\\n2. **People at high risk of cardiovascular disease**: Individuals with risk factors such as high blood pressure, high cholesterol, or diabetes may benefit from aspirin's anti-inflammatory and anti-clotting effects.\\n3. **People with atrial fibrillation**: Aspirin may help reduce the risk of stroke in people with atrial fibrillation.\\n\\n**Important notes**\\n\\n1. **Talk to your doctor**: Before starting aspirin therapy, discuss your individual risks and benefits with your healthcare provider.\\n2. **Low-dose aspirin**: Take low-dose aspirin (81 mg or 100 mg per day) to minimize bleeding risks.\\n3. **Regular monitoring**: Regularly check your blood pressure, kidney function, and liver function while taking aspirin.\\n\\nIn conclusion, aspirin can be beneficial for cardiovascular health in certain individuals, but it's essential to weigh the benefits against the potential risks and consult with a healthcare professional to determine the best course of action.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee8447",
   "metadata": {},
   "source": [
    "## Saving your fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79be4f0-d542-4740-8cfb-4b802de9b412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 7.55 out of 30.89 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 3/32 [00:00<00:02, 11.16it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:58<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"model\", tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
